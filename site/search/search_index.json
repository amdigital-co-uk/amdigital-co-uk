{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Platform Development Playbook This document... Continue etc.","title":"Home"},{"location":"#welcome-to-the-platform-development-playbook","text":"This document...","title":"Welcome to the Platform Development Playbook"},{"location":"#continue","text":"etc.","title":"Continue"},{"location":"1.-Welcome/Mission/","text":"Ensure the fast, predictable, and uninterrupted flow of planned work That delivers value to the business By delivering stable and secure software, and minimising the impact of unplanned work.","title":"Mission"},{"location":"1.-Welcome/Mission/#ensure-the-fast-predictable-and-uninterrupted-flow-of-planned-work","text":"","title":"Ensure the fast, predictable, and uninterrupted flow of planned work"},{"location":"1.-Welcome/Mission/#that-delivers-value-to-the-business","text":"","title":"That delivers value to the business"},{"location":"1.-Welcome/Mission/#by-delivering-stable-and-secure-software-and-minimising-the-impact-of-unplanned-work","text":"","title":"By delivering stable and secure software, and minimising the impact of unplanned work."},{"location":"1.-Welcome/Onboarding/","text":"Who is this for? This is the place to start when you join Platform Development at AM. It is designed for new starters, regardless of their role. It will point you to key areas of the playbook, to provide a quick overview of the department and how we work. Much of this should be covered during your induction. The playbook is your companion from induction to mastery. Initial Reading The Platform Development Division have a unified mission . Everything we do aims to aid us in achieving this mission. Platform Development is organised into a set of teams And we operate within an end to end delivery framework The delivery teams operate in a sprint model following the Scrum framework We build and maintain the Quartex , internal tooling, and legacy platforms. Check out the Knowledge Base in Azure DevOps and the Roadmaps. Development Practices This section will be most relevant to Software Engineers, but all team members are encouraged to read. // TODO Testing Practices This section will be most relevant to Test Engineers, but all team members are encouraged to read. // TODO Releasing Practices // TODO Product Management & Design Practices // TODO Tools and Access Tooling and platform directories live in the Knowledge Base. Therein you should find details of the tools and accounts you will likely require access to.","title":"Onboarding"},{"location":"1.-Welcome/Onboarding/#who-is-this-for","text":"This is the place to start when you join Platform Development at AM. It is designed for new starters, regardless of their role. It will point you to key areas of the playbook, to provide a quick overview of the department and how we work. Much of this should be covered during your induction. The playbook is your companion from induction to mastery.","title":"Who is this for?"},{"location":"1.-Welcome/Onboarding/#initial-reading","text":"The Platform Development Division have a unified mission . Everything we do aims to aid us in achieving this mission. Platform Development is organised into a set of teams And we operate within an end to end delivery framework The delivery teams operate in a sprint model following the Scrum framework We build and maintain the Quartex , internal tooling, and legacy platforms. Check out the Knowledge Base in Azure DevOps and the Roadmaps.","title":"Initial Reading"},{"location":"1.-Welcome/Onboarding/#development-practices","text":"This section will be most relevant to Software Engineers, but all team members are encouraged to read. // TODO","title":"Development Practices"},{"location":"1.-Welcome/Onboarding/#testing-practices","text":"This section will be most relevant to Test Engineers, but all team members are encouraged to read. // TODO","title":"Testing Practices"},{"location":"1.-Welcome/Onboarding/#releasing-practices","text":"// TODO","title":"Releasing Practices"},{"location":"1.-Welcome/Onboarding/#product-management-design-practices","text":"// TODO","title":"Product Management &amp; Design Practices"},{"location":"1.-Welcome/Onboarding/#tools-and-access","text":"Tooling and platform directories live in the Knowledge Base. Therein you should find details of the tools and accounts you will likely require access to.","title":"Tools and Access"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/","text":"General Guidelines for Documentation Including the correct information and putting it in the right place Firstly, review the section on Documentation Types, and follow those guidelines! Generally, some documentation is better than no documentation. It doesn't need to be perfect or complete straight away! Always include information about when the document was first written AND last reviewed. Knowing the correct audience Include, at the start, who the target audience is, and write documentation that will be understood by that audience. Consider the value that this audience will gain from it, and summarise this as the purpose of the document. If there is a need for technical and non-technical readers, consider breaking it up. How to keep it up to date Documentation should be reviewed periodically as most types of documentation have a fairly short lifespan. It is important that we know when we will next review each document, as such this should be determined when a document is published and when it is reviewed or updated. Obviously this does not apply to all types of document - such as code and commit comments. Repository readmes should always be reviewed and updated when related code is changed Ownership The creator / reviewer is responsible for determining the next review date // TODO: Need to identify document ownership strategy Reviewing Documentation Types We can perceive documentation as having different \"levels\" . From high level documentation covering ways of working with the widest audience, to lowest level documentation covering the function of modules or even lines of code. Within each level there are a number of types of documentation. Specific guidelines for each of these levels and their types are detailed in the subpages of this section of the wiki. Level Types Locations Department & Team Level Process and practice guidance, standards, onboarding The Platform Development Playbook Platform Level Architecture, infrastructure, patterns, developer guides Knowledge Base, Non-functional requirements Business Value & Functional level Functional requirements, Strategy, Roadmap items Functional guides, Roadmaps, Backlog Items & Feature Files Solution/Repository Level Solution overview, Solution-specific architecture & coding patterns, Build & run guides, Dependencies and known issues, Change & release logs ReadMe files, Code commit messages, Release plans, Knowledge Base Interface Level UI user guides, API developer guides User guides, Endpoint comments/Swagger(etc.) API documentation Code Level Code comments Classes and methods","title":"Documentation Guidelines"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#general-guidelines-for-documentation","text":"","title":"General Guidelines for Documentation"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#including-the-correct-information-and-putting-it-in-the-right-place","text":"Firstly, review the section on Documentation Types, and follow those guidelines! Generally, some documentation is better than no documentation. It doesn't need to be perfect or complete straight away! Always include information about when the document was first written AND last reviewed.","title":"Including the correct information and putting it in the right place"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#knowing-the-correct-audience","text":"Include, at the start, who the target audience is, and write documentation that will be understood by that audience. Consider the value that this audience will gain from it, and summarise this as the purpose of the document. If there is a need for technical and non-technical readers, consider breaking it up.","title":"Knowing the correct audience"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#how-to-keep-it-up-to-date","text":"Documentation should be reviewed periodically as most types of documentation have a fairly short lifespan. It is important that we know when we will next review each document, as such this should be determined when a document is published and when it is reviewed or updated. Obviously this does not apply to all types of document - such as code and commit comments. Repository readmes should always be reviewed and updated when related code is changed","title":"How to keep it up to date"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#ownership","text":"The creator / reviewer is responsible for determining the next review date // TODO: Need to identify document ownership strategy","title":"Ownership"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#reviewing","text":"","title":"Reviewing"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#documentation-types","text":"We can perceive documentation as having different \"levels\" . From high level documentation covering ways of working with the widest audience, to lowest level documentation covering the function of modules or even lines of code. Within each level there are a number of types of documentation. Specific guidelines for each of these levels and their types are detailed in the subpages of this section of the wiki. Level Types Locations Department & Team Level Process and practice guidance, standards, onboarding The Platform Development Playbook Platform Level Architecture, infrastructure, patterns, developer guides Knowledge Base, Non-functional requirements Business Value & Functional level Functional requirements, Strategy, Roadmap items Functional guides, Roadmaps, Backlog Items & Feature Files Solution/Repository Level Solution overview, Solution-specific architecture & coding patterns, Build & run guides, Dependencies and known issues, Change & release logs ReadMe files, Code commit messages, Release plans, Knowledge Base Interface Level UI user guides, API developer guides User guides, Endpoint comments/Swagger(etc.) API documentation Code Level Code comments Classes and methods","title":"Documentation Types"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/","text":"Who is this for? This playbook outlines the continually evolving frameworks, processes, and practices that are used by the Platform Development Division to enable us to reliably deliver against our collective mission . All members of the Platform Development Division - Product Management, Software Engineering, Test Engineering, Platform Support. Ownership and Responsibility Every member of the division is responsible for ensuring that this playbook is up to date and accurate. When you create, review, or modify a document, you are responsible for ensuring that it is well written and has been peer reviewed, and that review date is set. You must always provide a page header , using the following YAML heading set at the very top of each document in the playbook. --- title: authors: - Firstname Lastname - Firstname Lastname reviewed: yyyy-mm-dd reviewer: next-review: --- Structure // TODO : What are the structures that should be used (subpages, headings, TOCs, tags etc) Reviewing Documents should be peer reviewed after initial publication, and following significant revisions. They should also be subject to periodic reviews to ensure they are up to date, relevant, and accurate. Peer Reviewing The primary purpose of a review is to ensure that the documentation is as useful as it can be. The peer reviewer should carefully read and understand the content of the document. Consider the audience of the document and its purpose. Ensure the page header is populated and accurate, and that a review date has been provided. Check for typos and errors if possible, although this is not the primary purpose of review. Provide constructive feedback to the author for revision. Periodic Reviews On or around the review date, the document should be checked for validity. The purpose of a review is to ensure that the document is still as useful as it can be, or deleted if it is no longer required. Updates may happen at any time, but it is possible that changes to frameworks, processes, and practices have happened and not been documented. After a review, ensure that the Next Review Date in the page header is updated, regardless of whether any changes have been made. If a major revision is performed, add the name of the revising author(s) to the page header, and ensure that the revision is peer reviewed.","title":"Contributing to the playbook"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#who-is-this-for","text":"This playbook outlines the continually evolving frameworks, processes, and practices that are used by the Platform Development Division to enable us to reliably deliver against our collective mission . All members of the Platform Development Division - Product Management, Software Engineering, Test Engineering, Platform Support.","title":"Who is this for?"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#ownership-and-responsibility","text":"Every member of the division is responsible for ensuring that this playbook is up to date and accurate. When you create, review, or modify a document, you are responsible for ensuring that it is well written and has been peer reviewed, and that review date is set. You must always provide a page header , using the following YAML heading set at the very top of each document in the playbook. --- title: authors: - Firstname Lastname - Firstname Lastname reviewed: yyyy-mm-dd reviewer: next-review: ---","title":"Ownership and Responsibility"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#structure","text":"// TODO : What are the structures that should be used (subpages, headings, TOCs, tags etc)","title":"Structure"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#reviewing","text":"Documents should be peer reviewed after initial publication, and following significant revisions. They should also be subject to periodic reviews to ensure they are up to date, relevant, and accurate.","title":"Reviewing"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#peer-reviewing","text":"The primary purpose of a review is to ensure that the documentation is as useful as it can be. The peer reviewer should carefully read and understand the content of the document. Consider the audience of the document and its purpose. Ensure the page header is populated and accurate, and that a review date has been provided. Check for typos and errors if possible, although this is not the primary purpose of review. Provide constructive feedback to the author for revision.","title":"Peer Reviewing"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#periodic-reviews","text":"On or around the review date, the document should be checked for validity. The purpose of a review is to ensure that the document is still as useful as it can be, or deleted if it is no longer required. Updates may happen at any time, but it is possible that changes to frameworks, processes, and practices have happened and not been documented. After a review, ensure that the Next Review Date in the page header is updated, regardless of whether any changes have been made. If a major revision is performed, add the name of the revising author(s) to the page header, and ensure that the revision is peer reviewed.","title":"Periodic Reviews"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/API-Documentation/","text":"","title":"API Documentation"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Backlogs-and-requirements/","text":"","title":"Backlogs and requirements"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Comments-in-commits/","text":"","title":"Comments in commits"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Comments-within-code/","text":"","title":"Comments within code"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Readme-files-in-code-repositories/","text":"","title":"Readme files in code repositories"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Release-documentation/","text":"","title":"Release documentation"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/User-Guides/","text":"","title":"User Guides"},{"location":"2.-Delivery-Framework/","text":"Purpose Provide an overview of the delivery framework and its stages and their purpose, including inputs, outputs and guidelines for success. Audience All members of the Platform Development Division - Product Management, Software Engineering, Test Engineering, Platform Support. This may also be of interest to any non-technical stakeholder from across the business. Overview Our agile Delivery Framework is designed as a robust and clear abstract mechanism by which value is derived, delivered, and maintained by the organisation. While the high-level framework itself is fixed, the processes and practices that live within it should be constantly evolving, such that the framework continues to support our mission . You can see the detailed view of each of the process states by viewing the Miro Board","title":"Delivery Framework"},{"location":"2.-Delivery-Framework/#purpose","text":"Provide an overview of the delivery framework and its stages and their purpose, including inputs, outputs and guidelines for success.","title":"Purpose"},{"location":"2.-Delivery-Framework/#audience","text":"All members of the Platform Development Division - Product Management, Software Engineering, Test Engineering, Platform Support. This may also be of interest to any non-technical stakeholder from across the business.","title":"Audience"},{"location":"2.-Delivery-Framework/#overview","text":"Our agile Delivery Framework is designed as a robust and clear abstract mechanism by which value is derived, delivered, and maintained by the organisation. While the high-level framework itself is fixed, the processes and practices that live within it should be constantly evolving, such that the framework continues to support our mission . You can see the detailed view of each of the process states by viewing the Miro Board","title":"Overview"},{"location":"2.-Delivery-Framework/Topology-of-Business-Value/","text":"The Types of Backlog Item Software delivery is more than building features or working through a backlog of \"user stories\". Instead, we can should consider that everything we build ties directly to creating business value . // TODO: Add in topology diagram, rationale and context","title":"Topology of Business Value"},{"location":"2.-Delivery-Framework/Topology-of-Business-Value/#the-types-of-backlog-item","text":"Software delivery is more than building features or working through a backlog of \"user stories\". Instead, we can should consider that everything we build ties directly to creating business value . // TODO: Add in topology diagram, rationale and context","title":"The Types of Backlog Item"},{"location":"3.-Sprints-%26-Teams/","text":"Who is this for? This section and its subpages outline the \"Scrum\" delivery model, including outlining the structures, mechanisms and responsibilities therein. This should be read and understood by all members of the Platform Development department, not just the Product Delivery Team members. Introduction Currently we use the Scrum Framework to enable agile delivery. The documentation here is designed to extend and slightly adapt the core Scrum framework of practices to best suit our mission. You should therefore be already familiar with Scrum and it is recommended that you read the Scrum Guide . You should also have a solid understanding of Agile and DevOps theory and the value they can bring, this will be highly advantageous for the success of your team and the department as a whole. Scrum is not Agile, and Agile is not Scrum. They are distinct. Scrum outlines an approach to working in an Agile way, but being Agile requires an Agile mindset, which requires an understanding of its core principals and value proposition. The department currently is structured into two team types, including multiple Product Delivery Teams and a single Enablement Team Sprint Model The sprint includes a series of events that occur at the same time each day/sprint. Besides the key sprint events, there are other events that may occur periodically: - to support the design & definition of future work - currently as Backlog Refinement (although this will be reviewed as the new 3 Amigos practice is implemented) - to plan the runway of work for near future sprints - Runway planning Guidance and details on these events can be found in subpages. Product Delivery Teams These are small, cross functional, self organised \"Scrum teams\" that design, define, build, test, and release product increments that bring value to the business. These are described as product backlog items . Each team will consist of software engineers, test engineers, and a product manager. Product Delivery Teams (short hand: \"delivery teams\") will engage in both \"Sprint work\"- i.e. building, testing, and releasing product increments - and also preparation work - i.e. the design and definition (aka refinement) of future sprint work. Product Delivery Team Responsibilities Each of these teams has specific responsibilities that align with our mission : Sprint Success Determining which of the sprint Candidates can be achieved during the sprint, given the available resources and work that needs to be done (considering the Definition of Done), and communicating this to the Enablement Team Planning how this work will be done, including determining what work needs to be done and devising a strategy for completing it Identify blockers and risks that may prevent sprint success, and promptly communicating them to the Enablement Team Executing the planned work to the quality standards outlined in the Playbook Updating the plan as required, and communicating updates to the Enablement Team Planning and performing releases, and collaborating with the enablement team and other product delivery teams to minimise risk Measuring and reporting on the sprint progress Continued improvement of sprint effectiveness Reflecting upon and reporting on team effectiveness Identifying and implementing initiatives that help improve our overall effectiveness Measuring and reporting on the outcome of such initiatives, positive or negative Delivery teams must communicate the following information to the wider department (i.e. other delivery teams, stakeholders etc) Confirmation of sprint backlog (items and sprint priority) and goal following sprint planning Details of blockers and risks as and when they arise Sprint progress Changes to the sprint backlog should the plan needs to be updated Overview of sprint outcomes, including stories and total points delivered","title":"Sprints & Teams"},{"location":"3.-Sprints-%26-Teams/#who-is-this-for","text":"This section and its subpages outline the \"Scrum\" delivery model, including outlining the structures, mechanisms and responsibilities therein. This should be read and understood by all members of the Platform Development department, not just the Product Delivery Team members.","title":"Who is this for?"},{"location":"3.-Sprints-%26-Teams/#introduction","text":"Currently we use the Scrum Framework to enable agile delivery. The documentation here is designed to extend and slightly adapt the core Scrum framework of practices to best suit our mission. You should therefore be already familiar with Scrum and it is recommended that you read the Scrum Guide . You should also have a solid understanding of Agile and DevOps theory and the value they can bring, this will be highly advantageous for the success of your team and the department as a whole. Scrum is not Agile, and Agile is not Scrum. They are distinct. Scrum outlines an approach to working in an Agile way, but being Agile requires an Agile mindset, which requires an understanding of its core principals and value proposition. The department currently is structured into two team types, including multiple Product Delivery Teams and a single Enablement Team","title":"Introduction"},{"location":"3.-Sprints-%26-Teams/#sprint-model","text":"The sprint includes a series of events that occur at the same time each day/sprint. Besides the key sprint events, there are other events that may occur periodically: - to support the design & definition of future work - currently as Backlog Refinement (although this will be reviewed as the new 3 Amigos practice is implemented) - to plan the runway of work for near future sprints - Runway planning Guidance and details on these events can be found in subpages.","title":"Sprint Model"},{"location":"3.-Sprints-%26-Teams/#product-delivery-teams","text":"These are small, cross functional, self organised \"Scrum teams\" that design, define, build, test, and release product increments that bring value to the business. These are described as product backlog items . Each team will consist of software engineers, test engineers, and a product manager. Product Delivery Teams (short hand: \"delivery teams\") will engage in both \"Sprint work\"- i.e. building, testing, and releasing product increments - and also preparation work - i.e. the design and definition (aka refinement) of future sprint work.","title":"Product Delivery Teams"},{"location":"3.-Sprints-%26-Teams/#product-delivery-team-responsibilities","text":"Each of these teams has specific responsibilities that align with our mission : Sprint Success Determining which of the sprint Candidates can be achieved during the sprint, given the available resources and work that needs to be done (considering the Definition of Done), and communicating this to the Enablement Team Planning how this work will be done, including determining what work needs to be done and devising a strategy for completing it Identify blockers and risks that may prevent sprint success, and promptly communicating them to the Enablement Team Executing the planned work to the quality standards outlined in the Playbook Updating the plan as required, and communicating updates to the Enablement Team Planning and performing releases, and collaborating with the enablement team and other product delivery teams to minimise risk Measuring and reporting on the sprint progress Continued improvement of sprint effectiveness Reflecting upon and reporting on team effectiveness Identifying and implementing initiatives that help improve our overall effectiveness Measuring and reporting on the outcome of such initiatives, positive or negative Delivery teams must communicate the following information to the wider department (i.e. other delivery teams, stakeholders etc) Confirmation of sprint backlog (items and sprint priority) and goal following sprint planning Details of blockers and risks as and when they arise Sprint progress Changes to the sprint backlog should the plan needs to be updated Overview of sprint outcomes, including stories and total points delivered","title":"Product Delivery Team Responsibilities"},{"location":"3.-Sprints-%26-Teams/Daily-Scrum/","text":"Ahh! This one hasn't been written yet! Maybe you could do it?","title":"Daily Scrum"},{"location":"3.-Sprints-%26-Teams/Planning/","text":"Guide Objective: Determine a Sprint Goal and Sprint Backlog, and a plan for achieving it Organiser/Owner: Delivery Team Key stakeholders (required attendees): Delivery Team Scheduled: Start of the first day of the sprint Frequency: 1 per sprint Team split: 1 per delivery team Overview The team will review the candidates, their capacity for the sprint, then determine what work is required to complete the each of the candidates. They will estimate in hours each of the discrete work items that need to be performed. They will work in priority order, and stop planning when capacity is nearly matched by planned work. If the next priority item is deemed too large, the team may seek a smaller, lower priority candidate that can fit into the sprint. Agenda Review capacity - planned absences, known availability etc. Review candidates at a high level Speculate Sprint Goal Start planning each item, determine approximate order of tasks When capacity is filled and Sprint Backlog is finalised, discuss and align on plan for executing work Finalise Sprint Goal Communicate goal and backlog to Enablement team Tips for Success Be realistic about hours estimates. Optimism is likely to cause failure. Review past tasks to determine and improve estimation accuracy. Everyone in the team needs to leave with a clear understanding of the plan. DO NOT ASSUME that because someone has explained something that everyone has understood it. Speak out if you don't understand, you are equally responsible for ensuring that you leave the meeting with a good understanding of what needs to be done. If understanding of a problem is limited, plan right from the start to work in a way that improves team understanding, such as pair programming.","title":"Sprint Planning"},{"location":"3.-Sprints-%26-Teams/Planning/#overview","text":"The team will review the candidates, their capacity for the sprint, then determine what work is required to complete the each of the candidates. They will estimate in hours each of the discrete work items that need to be performed. They will work in priority order, and stop planning when capacity is nearly matched by planned work. If the next priority item is deemed too large, the team may seek a smaller, lower priority candidate that can fit into the sprint.","title":"Overview"},{"location":"3.-Sprints-%26-Teams/Planning/#agenda","text":"Review capacity - planned absences, known availability etc. Review candidates at a high level Speculate Sprint Goal Start planning each item, determine approximate order of tasks When capacity is filled and Sprint Backlog is finalised, discuss and align on plan for executing work Finalise Sprint Goal Communicate goal and backlog to Enablement team","title":"Agenda"},{"location":"3.-Sprints-%26-Teams/Planning/#tips-for-success","text":"Be realistic about hours estimates. Optimism is likely to cause failure. Review past tasks to determine and improve estimation accuracy. Everyone in the team needs to leave with a clear understanding of the plan. DO NOT ASSUME that because someone has explained something that everyone has understood it. Speak out if you don't understand, you are equally responsible for ensuring that you leave the meeting with a good understanding of what needs to be done. If understanding of a problem is limited, plan right from the start to work in a way that improves team understanding, such as pair programming.","title":"Tips for Success"},{"location":"3.-Sprints-%26-Teams/Retrospective/","text":"Guide Objective: Inspect team effectiveness and identify areas for improvement Organiser/Owner: Delivery Team Key stakeholders (required attendees): All delivery team members Scheduled: Occurs the last day of the sprint Frequency: 1 per sprint Team split: 1 per delivery team, offset if possible Overview Each delivery team should inspect their working practices to understand ways in which they can be adapted to help improve overall effectiveness (i.e. quality, speed, sustainability). Scrum is founded upon empirical practices and outlines three pillars of empiricism: Transparency, Inspection, and Adaptation. https://scrumguides.org/scrum-guide.html#scrum-theory The team should aim to 1. use facts (i.e. data) 1. in order to draw insights (i.e. ideas about causes) 1. and then create experiments (i.e. actions that may resolve/improve effectiveness in the specific area). Suggested Agenda Review sprint effectiveness data. Ideally this will be the start. Review existing experiments, identify experiments that are due for review, and ensure that ongoing experiments are being performed/measured well. Explore new areas for improvement to produce new experiments. Use games and workshops to derive ideas and insights from all team members. Creating and Managing Experiments Creating Experiments must be measurable, and the team must plan how and when measurements will be taken and reviewed, to understand if the experiment was successful. Capture the experiments you are performing, for example in a Excel spreadsheet or Miro board. Capture: - Reason - i.e. because of [perceived problem (including supporting data)] - Insight - i.e. we believe this is because of [cause] - Hypotheses - i.e. we believe this can be solved/improved if we do [potential solution, plan of action] - Measures - i.e. we will monitor/record [measurable outcomes] at [expecific intervals/times] - Review point - i.e. we will review success [date/after _n sprints]_ - Actions - i.e. things that must therefore be done by specific people/groups Note: - Be careful not to have too many experiments running at the same time. It may become hard to keep on top of them all - Be careful that your experiments don't contradict, negatively impact, or skew the measurements of other experiments Reviewing When you come to review the success of an experiment, you may decide to either: 1. Extend it, because it isn't clear what the result is yet 2. Stop it, because it was successful , and continue to perform this new behaviour 3. Stop it, because it was unsuccessful , and consider reviewing the insights drawn and then creating a new experiment 4. Pause it, because something is affecting your ability to perform the experiment well Remember to : - record the outcome, including new review dates etc. - update documentation when you establish new behaviours. Tips for Success Consider what data you might need to inspect and work to ensure that that data is available to you. The enablement team can help to ensure this is done. Attend the Retrospectives of the other team(s) from time to time, to learn about other approaches and what experiments are being conducted elsewhere. Invite others (from the other team or the enablement team) to facilitate your retro occasionally. It can help to ensure all team members have an equal voice if an outsider plans and runs it. Mix it up now and then! Try different techniques, games, etc. as you explore your effectiveness and look for areas to improve. As with all meetings, make sure that virtual/remote participants voices are heard. As a remote participant you are equally responsible for being heard and participating actively.","title":"Sprint Retrospective"},{"location":"3.-Sprints-%26-Teams/Retrospective/#overview","text":"Each delivery team should inspect their working practices to understand ways in which they can be adapted to help improve overall effectiveness (i.e. quality, speed, sustainability). Scrum is founded upon empirical practices and outlines three pillars of empiricism: Transparency, Inspection, and Adaptation. https://scrumguides.org/scrum-guide.html#scrum-theory The team should aim to 1. use facts (i.e. data) 1. in order to draw insights (i.e. ideas about causes) 1. and then create experiments (i.e. actions that may resolve/improve effectiveness in the specific area).","title":"Overview"},{"location":"3.-Sprints-%26-Teams/Retrospective/#suggested-agenda","text":"Review sprint effectiveness data. Ideally this will be the start. Review existing experiments, identify experiments that are due for review, and ensure that ongoing experiments are being performed/measured well. Explore new areas for improvement to produce new experiments. Use games and workshops to derive ideas and insights from all team members.","title":"Suggested Agenda"},{"location":"3.-Sprints-%26-Teams/Retrospective/#creating-and-managing-experiments","text":"","title":"Creating and Managing Experiments"},{"location":"3.-Sprints-%26-Teams/Retrospective/#creating","text":"Experiments must be measurable, and the team must plan how and when measurements will be taken and reviewed, to understand if the experiment was successful. Capture the experiments you are performing, for example in a Excel spreadsheet or Miro board. Capture: - Reason - i.e. because of [perceived problem (including supporting data)] - Insight - i.e. we believe this is because of [cause] - Hypotheses - i.e. we believe this can be solved/improved if we do [potential solution, plan of action] - Measures - i.e. we will monitor/record [measurable outcomes] at [expecific intervals/times] - Review point - i.e. we will review success [date/after _n sprints]_ - Actions - i.e. things that must therefore be done by specific people/groups Note: - Be careful not to have too many experiments running at the same time. It may become hard to keep on top of them all - Be careful that your experiments don't contradict, negatively impact, or skew the measurements of other experiments","title":"Creating"},{"location":"3.-Sprints-%26-Teams/Retrospective/#reviewing","text":"When you come to review the success of an experiment, you may decide to either: 1. Extend it, because it isn't clear what the result is yet 2. Stop it, because it was successful , and continue to perform this new behaviour 3. Stop it, because it was unsuccessful , and consider reviewing the insights drawn and then creating a new experiment 4. Pause it, because something is affecting your ability to perform the experiment well Remember to : - record the outcome, including new review dates etc. - update documentation when you establish new behaviours.","title":"Reviewing"},{"location":"3.-Sprints-%26-Teams/Retrospective/#tips-for-success","text":"Consider what data you might need to inspect and work to ensure that that data is available to you. The enablement team can help to ensure this is done. Attend the Retrospectives of the other team(s) from time to time, to learn about other approaches and what experiments are being conducted elsewhere. Invite others (from the other team or the enablement team) to facilitate your retro occasionally. It can help to ensure all team members have an equal voice if an outsider plans and runs it. Mix it up now and then! Try different techniques, games, etc. as you explore your effectiveness and look for areas to improve. As with all meetings, make sure that virtual/remote participants voices are heard. As a remote participant you are equally responsible for being heard and participating actively.","title":"Tips for Success"},{"location":"3.-Sprints-%26-Teams/Review/","text":"Guide Objective: Present the outcome of our sprint to business stakeholders, to gain feedback and insight Organiser/Owner: Delivery Teams Key stakeholders (required attendees): Members of the business who would like to learn about what the sprint teams have been working on, particularly those with a stake in a current piece of work Scheduled: Occurs the last day of the sprint Frequency: 1 per sprint Team split: Shared Overview A review of work carried out by Platform Development in the latest 2-week sprint, including demos of bug fixes and new functionality ahead of release. The review gives all interested parties an opportunity to inspect what has been built to date and time to ask questions, make observations or suggestions, and have discussions about how to best move forward. The review helps ensure we're building successful product and gives us the opportunity to get feedback from the business stakeholders who are typically not available on a daily basis. Preparation Guidelines Ahead of the Review meeting: - Complete the Sprint Review Template -- Planned Work = PBI / Type of Work (e.g. Feature, Bug, Tech Improvement) / Points / Status / Release (Name & Date where known) / Demo'd by -- Unplanned Work = Unplanned Backlog Items / Points / Status / Release (Name & Date where known) / Demo'd by - Agree a member of the Platform Development team to facilitate the meeting - Agree a member of each Delivery Team to talk through the sprint items - Discuss whether a demo is appropriate and if so, plan who will be responsible for this Agenda Overview of Sprint Sprint Name Sprint Dates Sprint Goal Points Completed Summary of Outcomes Planned Work -- Product Backlog Items associated with the sprint goal and who will benefit from this work -- Explanation where the sprint results do not match the sprint goal Unplanned Work -- Explanation of Unplanned Backlog Items addressed during sprint Product Demos Presentation of what was produced in the latest sprint Discussion Allow stakeholders to ask questions, understand the current state of the product and help guide its direction Feedback Team hear first hand if the stakeholders like what they see, if there are any changes they want and if an important feature has been missed Tips for Success All teams must present at every review to describe what has been accomplished and answer any questions Every team member should have the opportunity to present rather than the same person every time","title":"Sprint Review"},{"location":"3.-Sprints-%26-Teams/Review/#overview","text":"A review of work carried out by Platform Development in the latest 2-week sprint, including demos of bug fixes and new functionality ahead of release. The review gives all interested parties an opportunity to inspect what has been built to date and time to ask questions, make observations or suggestions, and have discussions about how to best move forward. The review helps ensure we're building successful product and gives us the opportunity to get feedback from the business stakeholders who are typically not available on a daily basis.","title":"Overview"},{"location":"3.-Sprints-%26-Teams/Review/#preparation-guidelines","text":"Ahead of the Review meeting: - Complete the Sprint Review Template -- Planned Work = PBI / Type of Work (e.g. Feature, Bug, Tech Improvement) / Points / Status / Release (Name & Date where known) / Demo'd by -- Unplanned Work = Unplanned Backlog Items / Points / Status / Release (Name & Date where known) / Demo'd by - Agree a member of the Platform Development team to facilitate the meeting - Agree a member of each Delivery Team to talk through the sprint items - Discuss whether a demo is appropriate and if so, plan who will be responsible for this","title":"Preparation Guidelines"},{"location":"3.-Sprints-%26-Teams/Review/#agenda","text":"","title":"Agenda"},{"location":"3.-Sprints-%26-Teams/Review/#overview-of-sprint","text":"Sprint Name Sprint Dates Sprint Goal Points Completed","title":"Overview of Sprint"},{"location":"3.-Sprints-%26-Teams/Review/#summary-of-outcomes","text":"Planned Work -- Product Backlog Items associated with the sprint goal and who will benefit from this work -- Explanation where the sprint results do not match the sprint goal Unplanned Work -- Explanation of Unplanned Backlog Items addressed during sprint","title":"Summary of Outcomes"},{"location":"3.-Sprints-%26-Teams/Review/#product-demos","text":"Presentation of what was produced in the latest sprint","title":"Product Demos"},{"location":"3.-Sprints-%26-Teams/Review/#discussion","text":"Allow stakeholders to ask questions, understand the current state of the product and help guide its direction","title":"Discussion"},{"location":"3.-Sprints-%26-Teams/Review/#feedback","text":"Team hear first hand if the stakeholders like what they see, if there are any changes they want and if an important feature has been missed","title":"Feedback"},{"location":"3.-Sprints-%26-Teams/Review/#tips-for-success","text":"All teams must present at every review to describe what has been accomplished and answer any questions Every team member should have the opportunity to present rather than the same person every time","title":"Tips for Success"},{"location":"3.-Sprints-%26-Teams/Runway-Planning/","text":"Guide Objective: Map out the deliverables for next several sprints Organiser/Owner: Product Delivery Teams Key stakeholders (required attendees): Anyone who wants something delivered in upcoming sprints Scheduled: Occurs the day before the sprint starts, or earlier. Frequency: Fortnightly Team split: 1 per delivery team, offset as enablement team required Overview Teams must look ahead and map out how the current highest priority backlog items will be delivered over the coming sprints (e.g. next 3-6 sprints). Each product delivery team has their own sprint runway plan. This will be a dynamic plan, not a fix plan. Doing so enables the team to understand the impact of pushing work out of the current sprint - i.e. if we can't do this now, when will we, and what else will be impacted. This supports the team's autonomy, and empowers them to make informed decisions about the best way to complete work. Inputs (e.g. things that have happened before the session) Stakeholders - which includes all members of the department - should add items to the backlog, they may speculatively add items into future sprints as well. Backlog items will be varying degrees of ready. The more ready it is, the more likely it can be addressed soon! The existing sprint runway plan forms the starting point. Each session refines and extends this plan. Activities & Outcomes If a stakeholder has added something to a sprint, they should use this session to explain why it is important to be done at this time. Together the team can discuss the impact of this item: what needs to be moved to accommodate it, or should this be pushed back/moved forwards. The team can add more items into sprints, from the backlog. Anyone is welcome to make suggestions. By the end of the session the team should be aligned on what is likely to be in the upcoming sprints - this is their \"sprint runway plan\". It should cover 3 - 6 sprints. The team should have a high level of confidence in the achievability of the very next sprint. Further ahead each sprints will gradually be less certain. Relative Sprint Priority Priority of backlog items should be ranked using standard criteria, such as impact and urgency, and should not be subjective. Relative Sprint Priority ranks sprint deliverables, i.e. backlog items within the context of a sprint. For example, multiple backlog items may have P2 or P3 priority against them, so they can be ordered in a way that indicated the priority within the context of this specific sprint. Equally, given wider implications (such as blockers or dependencies) it may be preferable or necessary to work on a lower priority item before a higher one. The product delivery team are responsible for determining their own Relative Sprint Priority. Stakeholders may of course offer their feedback. Tips for Success Remember that you are planning to be dynamic, not creating a rigid plan. Even though you are planning speculatively, you should try to be realistic. Don't overfill upcoming sprints. It will be tempting when stakeholders are all wanting competing work to be complete, but the team understand their competency better than anyone else, so they must communicate what they feel is achievable, and not just what they think stakeholders want to hear. Good estimation will help with good planning. It is important that we measure the success and impact of our estimates. The closer to \"Ready\" upcoming work is, the easier it will be to plan for it. Make sure you set aside time to get upcoming work ready .","title":"Runway Planning"},{"location":"3.-Sprints-%26-Teams/Runway-Planning/#overview","text":"Teams must look ahead and map out how the current highest priority backlog items will be delivered over the coming sprints (e.g. next 3-6 sprints). Each product delivery team has their own sprint runway plan. This will be a dynamic plan, not a fix plan. Doing so enables the team to understand the impact of pushing work out of the current sprint - i.e. if we can't do this now, when will we, and what else will be impacted. This supports the team's autonomy, and empowers them to make informed decisions about the best way to complete work.","title":"Overview"},{"location":"3.-Sprints-%26-Teams/Runway-Planning/#inputs","text":"(e.g. things that have happened before the session) Stakeholders - which includes all members of the department - should add items to the backlog, they may speculatively add items into future sprints as well. Backlog items will be varying degrees of ready. The more ready it is, the more likely it can be addressed soon! The existing sprint runway plan forms the starting point. Each session refines and extends this plan.","title":"Inputs"},{"location":"3.-Sprints-%26-Teams/Runway-Planning/#activities-outcomes","text":"If a stakeholder has added something to a sprint, they should use this session to explain why it is important to be done at this time. Together the team can discuss the impact of this item: what needs to be moved to accommodate it, or should this be pushed back/moved forwards. The team can add more items into sprints, from the backlog. Anyone is welcome to make suggestions. By the end of the session the team should be aligned on what is likely to be in the upcoming sprints - this is their \"sprint runway plan\". It should cover 3 - 6 sprints. The team should have a high level of confidence in the achievability of the very next sprint. Further ahead each sprints will gradually be less certain.","title":"Activities &amp; Outcomes"},{"location":"3.-Sprints-%26-Teams/Runway-Planning/#relative-sprint-priority","text":"Priority of backlog items should be ranked using standard criteria, such as impact and urgency, and should not be subjective. Relative Sprint Priority ranks sprint deliverables, i.e. backlog items within the context of a sprint. For example, multiple backlog items may have P2 or P3 priority against them, so they can be ordered in a way that indicated the priority within the context of this specific sprint. Equally, given wider implications (such as blockers or dependencies) it may be preferable or necessary to work on a lower priority item before a higher one. The product delivery team are responsible for determining their own Relative Sprint Priority. Stakeholders may of course offer their feedback.","title":"Relative Sprint Priority"},{"location":"3.-Sprints-%26-Teams/Runway-Planning/#tips-for-success","text":"Remember that you are planning to be dynamic, not creating a rigid plan. Even though you are planning speculatively, you should try to be realistic. Don't overfill upcoming sprints. It will be tempting when stakeholders are all wanting competing work to be complete, but the team understand their competency better than anyone else, so they must communicate what they feel is achievable, and not just what they think stakeholders want to hear. Good estimation will help with good planning. It is important that we measure the success and impact of our estimates. The closer to \"Ready\" upcoming work is, the easier it will be to plan for it. Make sure you set aside time to get upcoming work ready .","title":"Tips for Success"},{"location":"3.-Sprints-%26-Teams/Team-Structure/","text":"Ahh! This one hasn't been written yet! Maybe you could do it?","title":"Team Structure"},{"location":"4.-Backlog-Management/","text":"Who is this for? This section describes the way in which we turn roadmap items into backlog items which are ready to be built. It is intended for anyone engaged in creating ready backlog items.","title":"Backlog Management"},{"location":"4.-Backlog-Management/#who-is-this-for","text":"This section describes the way in which we turn roadmap items into backlog items which are ready to be built. It is intended for anyone engaged in creating ready backlog items.","title":"Who is this for?"},{"location":"4.-Backlog-Management/Backlog-Item-Types/","text":"Overview A backlog is a list of discrete things we have determined we want to deliver, in order progress out roadmap. A roadmap item therefore has one or more backlog items that must be delivered. Backlog items deliver different types of things, and therefore take different forms. Understanding and distinguishing the type of backlog item helps us determine how to prepare it , estimate it, and plan for it. Once backlog items are ready, they can be prioritised within the Product Backlog so that they can be selected as candidates for upcoming sprints Type of Deliverable Purpose Process Features and Changes Deliver Product Roadmap items Defining Features and Changes Technical improvements Deliver Technical Roadmap items Defining Technical Improvements Bugs Progressed to 3rd Line Support from 2nd Line Support TBC Knowledge Acquisition Support our ability to define and ready the above types TBC","title":"Backlog Item Types"},{"location":"4.-Backlog-Management/Backlog-Item-Types/#overview","text":"A backlog is a list of discrete things we have determined we want to deliver, in order progress out roadmap. A roadmap item therefore has one or more backlog items that must be delivered. Backlog items deliver different types of things, and therefore take different forms. Understanding and distinguishing the type of backlog item helps us determine how to prepare it , estimate it, and plan for it. Once backlog items are ready, they can be prioritised within the Product Backlog so that they can be selected as candidates for upcoming sprints Type of Deliverable Purpose Process Features and Changes Deliver Product Roadmap items Defining Features and Changes Technical improvements Deliver Technical Roadmap items Defining Technical Improvements Bugs Progressed to 3rd Line Support from 2nd Line Support TBC Knowledge Acquisition Support our ability to define and ready the above types TBC","title":"Overview"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/","text":"Who is this for? Describes the way in which we turn roadmap items into ready backlog items. This document is indented for anyone engaged in creating ready backlog items. Quienes son los 3 Amigos? The term \"3 Amigos\" refers to the perspectives that are generally required to effectively define the requirements of a roadmap item, be it product, technical, or support: Perspective Identifier Business Perspective in the form of the \"Product Owner\", usually performed by a Product Manager for product roadmap items, support manager for support items etc, tech lead for tech items Engineering Perspective in the form of a \"Technical Lead\" (usually a software engineer or architect) Quality Assurance Perspective in the form of a \"Test Lead\" (usually a test engineer) As such each Roadmap item is assigned a Product Owner, a Technical Lead, and a Test Lead. Although there are typical business roles that map to the 3A role (indicated above), each of them can be assigned to any person who can perform the duties effectively. The three amigos approach aims to ensure that the smallest group of people possible will collaborate to create a ready backlog item. It is of course possible therefore that more than 3 people will be required, for example, a Design lead where there are UI components to deliver. The three Amigos will work with other stakeholders from across the business as they go through Definition phase. For example, the Tech Lead should consult with other engineers and the Architect. // TODO: Who should determine the 3 Amigos? How and When is this done? Responsibilities The Three Amigos are collectively responsible for: - creating ready backlog items that describe an effective solution to the problem - ensuring that ready backlog items are produced \"in time\", and communicating with the Head of Product about their progress and blockers. - ensuring that they engage with the right stakeholders - recording their activities and decisions - planning and executing the definition work - supporting the build and release phase by resolving queries and updating requirements // TODO: Set out the specific responsibilities of each of the 3A roles. Backlog Prioritisation // Todo: describe impact & urgency ranking Sprint Candidates // Todo: describe impact & urgency ranking","title":"3 Amigos & Readying Backlog Items"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/#who-is-this-for","text":"Describes the way in which we turn roadmap items into ready backlog items. This document is indented for anyone engaged in creating ready backlog items.","title":"Who is this for?"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/#quienes-son-los-3-amigos","text":"The term \"3 Amigos\" refers to the perspectives that are generally required to effectively define the requirements of a roadmap item, be it product, technical, or support: Perspective Identifier Business Perspective in the form of the \"Product Owner\", usually performed by a Product Manager for product roadmap items, support manager for support items etc, tech lead for tech items Engineering Perspective in the form of a \"Technical Lead\" (usually a software engineer or architect) Quality Assurance Perspective in the form of a \"Test Lead\" (usually a test engineer) As such each Roadmap item is assigned a Product Owner, a Technical Lead, and a Test Lead. Although there are typical business roles that map to the 3A role (indicated above), each of them can be assigned to any person who can perform the duties effectively. The three amigos approach aims to ensure that the smallest group of people possible will collaborate to create a ready backlog item. It is of course possible therefore that more than 3 people will be required, for example, a Design lead where there are UI components to deliver. The three Amigos will work with other stakeholders from across the business as they go through Definition phase. For example, the Tech Lead should consult with other engineers and the Architect. // TODO: Who should determine the 3 Amigos? How and When is this done?","title":"Quienes son los 3 Amigos?"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/#responsibilities","text":"The Three Amigos are collectively responsible for: - creating ready backlog items that describe an effective solution to the problem - ensuring that ready backlog items are produced \"in time\", and communicating with the Head of Product about their progress and blockers. - ensuring that they engage with the right stakeholders - recording their activities and decisions - planning and executing the definition work - supporting the build and release phase by resolving queries and updating requirements // TODO: Set out the specific responsibilities of each of the 3A roles.","title":"Responsibilities"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/#backlog-prioritisation","text":"// Todo: describe impact & urgency ranking","title":"Backlog Prioritisation"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/#sprint-candidates","text":"// Todo: describe impact & urgency ranking","title":"Sprint Candidates"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Accessibility-Impact-Assessment/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Accessibility Impact Assessment"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Architectural-Design/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Architectural Design"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Functional-Requirements-with-BDD/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Functional Requirements with BDD"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Non-Functional-Requirements/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Non Functional Requirements"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Privacy-Impact-Assessments/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Privacy Impact Assessments"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Problem%2C-Value%2C-Solution-Statements/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Problem, Value, Solution Statements"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Risk%2C-Assumptions%2C-Issues%2C-%26-Dependencies/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Risk, Assumptions, Issues, & Dependencies"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Security-Impact-Assessment/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Security Impact Assessment"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/UI-%26-UX-Designs/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"UI & UX Designs"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/User-Flow-Diagrams/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"User Flow Diagrams"},{"location":"4.-Backlog-Management/Definition-of-Ready/","text":"What's a Definition of Ready? A ready backlog item is one that can be immediately actioned (or worked on ) by a delivery team. That means it must have enough information in it in order for it to be delivered by the team fast and effectively. The Definition of Ready (DOR), therefore, outlines the characteristics and information - described as artefacts - that a delivery team would need in order to action an backlog item. However, caution must be taken to ensure that the DOR does not become a set of linear gates that effectively prevent us working in an agile way. We must balance the benefits of having a clear and aligned understanding of our objective that enables with being able to adapt quickly. Each backlog item type will have it's own DOR, and corressponding artefacts. 2 Key DORs are: Features and Changes Technical Improvements Information regarding producing each artefact type can be found within the 3 Amigos & Readying Backlog Items section. Note: The Definition of Ready should be considered a guide rather than a set of hard rules. The artefacts actually needed will vary depending on the nature of the backlog item. It is up to the 3 Amigos team to determine what readiness means for each backlog item as they go through the definition processes. Backlog Prioritisation // Todo: describe impact & urgency ranking Sprint Candidates // Todo: describe impact & urgency ranking","title":"Definition of Ready"},{"location":"4.-Backlog-Management/Definition-of-Ready/#whats-a-definition-of-ready","text":"A ready backlog item is one that can be immediately actioned (or worked on ) by a delivery team. That means it must have enough information in it in order for it to be delivered by the team fast and effectively. The Definition of Ready (DOR), therefore, outlines the characteristics and information - described as artefacts - that a delivery team would need in order to action an backlog item. However, caution must be taken to ensure that the DOR does not become a set of linear gates that effectively prevent us working in an agile way. We must balance the benefits of having a clear and aligned understanding of our objective that enables with being able to adapt quickly. Each backlog item type will have it's own DOR, and corressponding artefacts. 2 Key DORs are: Features and Changes Technical Improvements Information regarding producing each artefact type can be found within the 3 Amigos & Readying Backlog Items section. Note: The Definition of Ready should be considered a guide rather than a set of hard rules. The artefacts actually needed will vary depending on the nature of the backlog item. It is up to the 3 Amigos team to determine what readiness means for each backlog item as they go through the definition processes.","title":"What's a Definition of Ready?"},{"location":"4.-Backlog-Management/Definition-of-Ready/#backlog-prioritisation","text":"// Todo: describe impact & urgency ranking","title":"Backlog Prioritisation"},{"location":"4.-Backlog-Management/Definition-of-Ready/#sprint-candidates","text":"// Todo: describe impact & urgency ranking","title":"Sprint Candidates"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/","text":"Definition of Ready :smile: Ready Features and Change Backlog Items should: Meet INVEST criteria and Include the following artefacts: Problem Statement Value Proposition Solution Executive Summary User Flow diagrams UI & UX designs Functional Requirements , expressed as: BDD Scenarios BDD Business Rules Non-functional requirements including: Non-functional acceptance criteria (above and beyond platform-wide standards) Privacy Impact Assessment * Security Impact Assessment * Accessibility Impact Assessment * Architectural design Risks, Assumptions, Issues, and Dependencies Rationalisation - any sporting information that provides context to the artefacts above (for example, a decision log) Definition Process Flow // TODO: Determine diagramming needs. Is Mermaid sufficient? Should we link out to Miro, import images, or use something else? The process flow details an example of the way a roadmap item is understood and eventually turned into well defined backlog items. Note that the specific mechanisms are generally purposefully excluded, such that different techniques and tools can be used as desired by the team. What matters is that we are able to define great solutions that read for delivery teams as fast and effectively as possible, not how that comes about. Planning and Scoping \"Scoping\" of the roadmap item includes the solicitation of requirements from stakeholders and users, and the definition what the scope of deliverable. Generally, this includes who (which users) will be able to do what , and why . This would be accompanied by the specific needs of the users, as well as any specific exclusions (i.e. things that are out of scope). This information will inform the functional and non-functional requirements of resulting backlog items. Once the scope is known, the 3As can plan the specific activities they wish to perform as through the Definition phase - including how long this will take and when. Technical Scoping This stage include understanding technical scope. This is not to be confused with technical design, which comes later. At this point we know what problem we are trying to solve, and we are looking to understand the technology constraints. This could include: - 3rd party technologies specifically required - for example is there are pre-defined 3rd party API we must integrate with, and what are its limitations and benefits. - Other work that may block or impact. - Existing platform limitations or competencies. Initial Design The initial design stage covers researching and then creating one or more design options. Design does not necessarily mean creating visual designs. Again there are no fixed practices for this stage. Each problem will require its own approach and it is up to the 3As to agree on the approach taken. This is a great time to think about what additional knowledge could be needed. It's quite possible that a desirable solution raises a number of questions about technical feasibility. Do we need to embark on an investigation - such as creating a proof of concept, understanding some technical documentation, etc? Define high-level solution Armed with the scope and some potential solutions, a high level can be defined. A high level solution to a roadmap item should be summarised and, where appropriate, supported with UI designs and a technical overview. Low Res Design & User Flow \"Low Res\" refers to a low degree of detail, and example of this is wireframing. Coupled with user flow diagrams, these provide the backbone to our functional requirements. Create Backlog Items The 3As must split the roadmap item into backlog items. Initially this is done by creating \"shell\" backlog items with the basic information of what it will deliver, the following steps will progress from here to make them \"ready\". Turning large solutions into discrete deliverables is an art form! Our definition of ready provides some guidelines of what a backlog item should be, generally centred around the INVEST principals. Further reading and learning on this topic is highly recommended. High Res designs // TODO: complete high res designs section Functional and Non-Functional Requirements // TODO: complete Reqs section Technical approach // TODO: complete tech approach section Team Review // TODO: complete review section","title":"Features and Changes"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#definition-of-ready","text":":smile: Ready Features and Change Backlog Items should: Meet INVEST criteria and Include the following artefacts: Problem Statement Value Proposition Solution Executive Summary User Flow diagrams UI & UX designs Functional Requirements , expressed as: BDD Scenarios BDD Business Rules Non-functional requirements including: Non-functional acceptance criteria (above and beyond platform-wide standards) Privacy Impact Assessment * Security Impact Assessment * Accessibility Impact Assessment * Architectural design Risks, Assumptions, Issues, and Dependencies Rationalisation - any sporting information that provides context to the artefacts above (for example, a decision log)","title":"Definition of Ready"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#definition-process-flow","text":"// TODO: Determine diagramming needs. Is Mermaid sufficient? Should we link out to Miro, import images, or use something else? The process flow details an example of the way a roadmap item is understood and eventually turned into well defined backlog items. Note that the specific mechanisms are generally purposefully excluded, such that different techniques and tools can be used as desired by the team. What matters is that we are able to define great solutions that read for delivery teams as fast and effectively as possible, not how that comes about.","title":"Definition Process Flow"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#planning-and-scoping","text":"\"Scoping\" of the roadmap item includes the solicitation of requirements from stakeholders and users, and the definition what the scope of deliverable. Generally, this includes who (which users) will be able to do what , and why . This would be accompanied by the specific needs of the users, as well as any specific exclusions (i.e. things that are out of scope). This information will inform the functional and non-functional requirements of resulting backlog items. Once the scope is known, the 3As can plan the specific activities they wish to perform as through the Definition phase - including how long this will take and when.","title":"Planning and Scoping"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#technical-scoping","text":"This stage include understanding technical scope. This is not to be confused with technical design, which comes later. At this point we know what problem we are trying to solve, and we are looking to understand the technology constraints. This could include: - 3rd party technologies specifically required - for example is there are pre-defined 3rd party API we must integrate with, and what are its limitations and benefits. - Other work that may block or impact. - Existing platform limitations or competencies.","title":"Technical Scoping"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#initial-design","text":"The initial design stage covers researching and then creating one or more design options. Design does not necessarily mean creating visual designs. Again there are no fixed practices for this stage. Each problem will require its own approach and it is up to the 3As to agree on the approach taken. This is a great time to think about what additional knowledge could be needed. It's quite possible that a desirable solution raises a number of questions about technical feasibility. Do we need to embark on an investigation - such as creating a proof of concept, understanding some technical documentation, etc?","title":"Initial Design"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#define-high-level-solution","text":"Armed with the scope and some potential solutions, a high level can be defined. A high level solution to a roadmap item should be summarised and, where appropriate, supported with UI designs and a technical overview.","title":"Define high-level solution"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#low-res-design-user-flow","text":"\"Low Res\" refers to a low degree of detail, and example of this is wireframing. Coupled with user flow diagrams, these provide the backbone to our functional requirements.","title":"Low Res Design &amp; User Flow"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#create-backlog-items","text":"The 3As must split the roadmap item into backlog items. Initially this is done by creating \"shell\" backlog items with the basic information of what it will deliver, the following steps will progress from here to make them \"ready\". Turning large solutions into discrete deliverables is an art form! Our definition of ready provides some guidelines of what a backlog item should be, generally centred around the INVEST principals. Further reading and learning on this topic is highly recommended.","title":"Create Backlog Items"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#high-res-designs","text":"// TODO: complete high res designs section","title":"High Res designs"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#functional-and-non-functional-requirements","text":"// TODO: complete Reqs section","title":"Functional and Non-Functional Requirements"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#technical-approach","text":"// TODO: complete tech approach section","title":"Technical approach"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#team-review","text":"// TODO: complete review section","title":"Team Review"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Technical-Improvements/","text":"Definition of Ready Note: The definition of ready should be considered a guide rather than a set of hard rules. The artefacts actually needed will vary depending on the nature of the backlog item. It is up to the 3 Amigos team to determine what readiness means for each backlog item as they go through the definition processes. Ready Features and Change Backlog Items should: Meet INVEST criteria and Include the following artefacts: Problem Statement Value Proposition Solution Executive Summary Measures for success and mechanisms for producing metrics Technical requirements, such as: Behaviours needed from tools and teams Non-functional requirements including: Non-functional acceptance criteria (above and beyond platform-wide standards) Privacy Impact Assessment * Security Impact Assessment * Accessibility Impact Assessment * Architectural design Risks, Assumptions, Issues, and Dependencies Rationalisation - any sporting information that provides context to the artefacts above (for example, a decision log)","title":"Technical Improvements"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Technical-Improvements/#definition-of-ready","text":"Note: The definition of ready should be considered a guide rather than a set of hard rules. The artefacts actually needed will vary depending on the nature of the backlog item. It is up to the 3 Amigos team to determine what readiness means for each backlog item as they go through the definition processes. Ready Features and Change Backlog Items should: Meet INVEST criteria and Include the following artefacts: Problem Statement Value Proposition Solution Executive Summary Measures for success and mechanisms for producing metrics Technical requirements, such as: Behaviours needed from tools and teams Non-functional requirements including: Non-functional acceptance criteria (above and beyond platform-wide standards) Privacy Impact Assessment * Security Impact Assessment * Accessibility Impact Assessment * Architectural design Risks, Assumptions, Issues, and Dependencies Rationalisation - any sporting information that provides context to the artefacts above (for example, a decision log)","title":"Definition of Ready"},{"location":"5.-Rules-of-Engagement/","text":"Who is this for? This section will outline some of the processes and practices surrounding how we operate as a department, beyond those within our delivery framework. The aim is to ensure that we operate as effectively and reliably as possible. This section should be read and well understood by all members of Platform Development. These processes extend the business's HR processes, and do not replace them.","title":"Rules of Engagement"},{"location":"5.-Rules-of-Engagement/#who-is-this-for","text":"This section will outline some of the processes and practices surrounding how we operate as a department, beyond those within our delivery framework. The aim is to ensure that we operate as effectively and reliably as possible. This section should be read and well understood by all members of Platform Development. These processes extend the business's HR processes, and do not replace them.","title":"Who is this for?"},{"location":"5.-Rules-of-Engagement/Absence-%26-Leave/","text":"Sickness & Unplanned Leave Unplanned leave is any short notice leave that has not been booked in advance. Follow the HR guidelines on communicating unplanned absence. Additionally, if you are able to, send a message using Teams to key channels (e.g. the relevant Delivery Team channel, any 3As groups etc) to inform them that you are not working. This will help the team to rapidly re-plan any work as required. Engineers Tip - You should always commit and push changes to your working branch at the end of the working day, including descriptive comments about the work in progress. You can never guarantee you'll be in a position to pick up where you left off the next day! Annual Leave & Planned Leave !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. !!! note If you are planning to miss a Daily Scrum for any reason, you MUST make sure you provide an update of your WIP to the team **in advance** and that the Sprint Board is updated with your progress Requesting Annual Leave Before requesting annual leave, check the planner for the dates you want to book off. A first come, first serve approach is applied, so booking early is advised, especially for longer periods of absence. We need to make sure that each of the delivery teams is able to continue working in your absence, so if other members of your team are already booked off, then you need to have a conversation with your team about the impact of additional impact this might have. If you determine that the team can continue effectively with the remaining resource you are welcome to submit the request. Use the comments field in YouManage to detail the outcome of the conversation. Engineers: If you are on on the rota to act as Support Engineer during the period that you wish to book off, you should contact the Support Manager and assist them in adjusting the rota (for longer periods of absence) or finding cover (for shorter periods). DO NOT LEAVE IT TO YOUR LINE MANAGER TO WORK THIS OUT FOR YOU. The earlier you book the easier it will be to plan for your absence The Longer your absence period is, the more advanced notice you should give, as a guide you should aim to give AT LEAST 2 x the booking period. (e.g. 2 days off - 4 days notice, 2 weeks off - 4 weeks notice). If there are meetings during your planned absence you must notify the meeting organiser(s). If you are integral to the meeting you should help to reorganise, if you are not then ensure that you work with the organiser to prepare and communicate any required input in advance. This includes all sprint meeting ESPECIALLY THE DAILY SCRUM. Christmas Period Christmas period bookings will not be accepted until later in the year, when we will have a clearer understanding of the anticipated workload and resource availability during that period. First come first serve does not apply, as this often yields unfair results. Specific details of how to submit requests will be communicated with the team when the time comes. Usually around late September. Please do not request annual leave over the Christmas and New Years period via YouManage until this communication has been made. Preparing for Annual Leave Generally, you must ensure that anyone you work with regularly or are likely to be working with around the time of your leave is aware of your planned absence. Specifically, ensure that each of the following are performed: When your annual leave request has been approved, Ensure that your team are aware of this as soon as possible Take proactive steps to organise and perform any re-planning necessary Put the leave in your calendar, mark it as out of office Before the sprint (s) in which your annual leave occurs: If you are going to miss sprint planning, ensure that your capacity is recorded in advance or request a team member to do so for you. Ask the team to provide an update on the sprint plan on your return At the start of the sprint (s) in which your annual leave occurs: Ensure that your capacity is adjusted during planning Ensure that the sprint work is planned around your absence, especially where work depends on yourself. The day before your leave Set your out of office in Outlook and Teams Remind your team that you will be on leave and when you will be returning. Provide the team with an update of your work in progress and provide a handover if required. Engineers: you must ensure that any WIP code is committed to your working branch and pushed to the remote, including descriptive comments about the work in progress. Support Engineer: If you are currently acting as Support Engineer when your leave starts, ensure that you provide a clear and complete handover to the Support Manager and the engineer who is covering during your leave. Returning from Annual Leave Work with your team to get yourself up to speed. You are responsible for organising this. You must prioritise this and ensure that you understand the current sprint plan and progress. Hybrid Working Framework // TODO: link out here","title":"Absence and Leave"},{"location":"5.-Rules-of-Engagement/Absence-%26-Leave/#sickness-unplanned-leave","text":"Unplanned leave is any short notice leave that has not been booked in advance. Follow the HR guidelines on communicating unplanned absence. Additionally, if you are able to, send a message using Teams to key channels (e.g. the relevant Delivery Team channel, any 3As groups etc) to inform them that you are not working. This will help the team to rapidly re-plan any work as required. Engineers Tip - You should always commit and push changes to your working branch at the end of the working day, including descriptive comments about the work in progress. You can never guarantee you'll be in a position to pick up where you left off the next day!","title":"Sickness &amp; Unplanned Leave"},{"location":"5.-Rules-of-Engagement/Absence-%26-Leave/#annual-leave-planned-leave","text":"!!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. !!! note If you are planning to miss a Daily Scrum for any reason, you MUST make sure you provide an update of your WIP to the team **in advance** and that the Sprint Board is updated with your progress","title":"Annual Leave &amp; Planned Leave"},{"location":"5.-Rules-of-Engagement/Absence-%26-Leave/#requesting-annual-leave","text":"Before requesting annual leave, check the planner for the dates you want to book off. A first come, first serve approach is applied, so booking early is advised, especially for longer periods of absence. We need to make sure that each of the delivery teams is able to continue working in your absence, so if other members of your team are already booked off, then you need to have a conversation with your team about the impact of additional impact this might have. If you determine that the team can continue effectively with the remaining resource you are welcome to submit the request. Use the comments field in YouManage to detail the outcome of the conversation. Engineers: If you are on on the rota to act as Support Engineer during the period that you wish to book off, you should contact the Support Manager and assist them in adjusting the rota (for longer periods of absence) or finding cover (for shorter periods). DO NOT LEAVE IT TO YOUR LINE MANAGER TO WORK THIS OUT FOR YOU. The earlier you book the easier it will be to plan for your absence The Longer your absence period is, the more advanced notice you should give, as a guide you should aim to give AT LEAST 2 x the booking period. (e.g. 2 days off - 4 days notice, 2 weeks off - 4 weeks notice). If there are meetings during your planned absence you must notify the meeting organiser(s). If you are integral to the meeting you should help to reorganise, if you are not then ensure that you work with the organiser to prepare and communicate any required input in advance. This includes all sprint meeting ESPECIALLY THE DAILY SCRUM.","title":"Requesting Annual Leave"},{"location":"5.-Rules-of-Engagement/Absence-%26-Leave/#christmas-period","text":"Christmas period bookings will not be accepted until later in the year, when we will have a clearer understanding of the anticipated workload and resource availability during that period. First come first serve does not apply, as this often yields unfair results. Specific details of how to submit requests will be communicated with the team when the time comes. Usually around late September. Please do not request annual leave over the Christmas and New Years period via YouManage until this communication has been made.","title":"Christmas Period"},{"location":"5.-Rules-of-Engagement/Absence-%26-Leave/#preparing-for-annual-leave","text":"Generally, you must ensure that anyone you work with regularly or are likely to be working with around the time of your leave is aware of your planned absence. Specifically, ensure that each of the following are performed: When your annual leave request has been approved, Ensure that your team are aware of this as soon as possible Take proactive steps to organise and perform any re-planning necessary Put the leave in your calendar, mark it as out of office Before the sprint (s) in which your annual leave occurs: If you are going to miss sprint planning, ensure that your capacity is recorded in advance or request a team member to do so for you. Ask the team to provide an update on the sprint plan on your return At the start of the sprint (s) in which your annual leave occurs: Ensure that your capacity is adjusted during planning Ensure that the sprint work is planned around your absence, especially where work depends on yourself. The day before your leave Set your out of office in Outlook and Teams Remind your team that you will be on leave and when you will be returning. Provide the team with an update of your work in progress and provide a handover if required. Engineers: you must ensure that any WIP code is committed to your working branch and pushed to the remote, including descriptive comments about the work in progress. Support Engineer: If you are currently acting as Support Engineer when your leave starts, ensure that you provide a clear and complete handover to the Support Manager and the engineer who is covering during your leave.","title":"Preparing for Annual Leave"},{"location":"5.-Rules-of-Engagement/Absence-%26-Leave/#returning-from-annual-leave","text":"Work with your team to get yourself up to speed. You are responsible for organising this. You must prioritise this and ensure that you understand the current sprint plan and progress.","title":"Returning from Annual Leave"},{"location":"5.-Rules-of-Engagement/Absence-%26-Leave/#hybrid-working-framework","text":"// TODO: link out here","title":"Hybrid Working Framework"},{"location":"6.-Engineering/Engineering/","text":"This section contains an overview of practices, tools, principals, and patterns used by engineers in our teams, as well as guidelines for how to follow them. This is our \"house style\". All engineers must ensure that they understand and are able to follow these guidelines. For the most part they are not rules however. Guidelines must be followed until there is a reasonable case to not do so. In this instance you should ensure the guidelines are then updated if appropriate. Note: Specific implementation details should be included within the ReadMe files of the relevant repositories or subdirectories therein, however they may be referenced here for example purposes.","title":"Engineering"},{"location":"6.-Engineering/Managing-Technical-Debt/","text":"","title":"Managing Technical Debt"},{"location":"6.-Engineering/Peer-Reviewing/","text":"[[ TOC ]] Who is this for? Outlines the mechanisms by which engineers share their approach to a specific solution during development. Engineers - dev & test Overview The purpose of a peer review is to identify mistakes as early as possible and ensure that the way a solution is implemented is: 1. Understood by more than one member of the team - creating a shared understanding of our platform 1. Inline with our quality standards Peer reviewing must not be left until last minute, or performed as a one-time event. Instead, we should be working closely with our colleagues to share our approach throughout the development cycle. This allows for early feedback around the way a solution is built, and allows for the reviewer to gain a greater degree of context. The final stage of peer reviewing is when code is merged into a parent branch (e.g. Work Branch --> Feature Branch, or Feature Branch --> Release Branch). This process allows for in depth reviewing of the full solution. Pair Programming & WIP Reviewing Think about potential review points as a team during sprint planning! Creating discrete review tasks will help track progress and remind the team of the need to review. Pair programming, screen share walk throughs, or simply reviewing a work branch are some ways to perform peer reviews of WIP. Any member of the team can perform a peer review for anyone else, not just a more senior member! Providing Feedback for WIP Feedback should be given directly to the original engineer by the reviewer. The reviewer should feel free to ask questions and provide their honest perspective. It is wise to support feedback with rationale; in what way could the solution be improved? how will these improvements help us improve quality? The original engineer is not obliged to act upon all feedback, but an open discussion will be very beneficial. Reviews whilst pair programming A review should still be performed for any code written during a pair programming session. However, it may be helpful for the two engineers to perform the review together at the end of the pairing session. E.g. the engineer that has been screensharing/writing the code should check everything in, and then the other engineer lead the review. However both engineers would be encouraged to discuss whether or not the code they have written together fulfils the guidelines.= Submitting code for a review It is best to submit code to be reviewed frequently, both to get early feedback and to ensure that the reviewing engineer can get a good picture of the changes without being overwhelmed by a huge amount of changes all at once. In other words, follow the principle of little and often! When submitting a PR the engineer should follow the Pull Request Guidelines , and should ensure to provide some context of what the code being reviewed is try to achieve and how it is structured. If refactoring existing code (in preparation for subsequent changes), the refactoring should be performed separately and submitted for review in isolation. It would otherwise be much harder for the reviewing engineer to understand how the refactoring has been done if there are also additional changes and new functionality mixed in.","title":"Peer Reviewing"},{"location":"6.-Engineering/Peer-Reviewing/#who-is-this-for","text":"Outlines the mechanisms by which engineers share their approach to a specific solution during development. Engineers - dev & test","title":"Who is this for?"},{"location":"6.-Engineering/Peer-Reviewing/#overview","text":"The purpose of a peer review is to identify mistakes as early as possible and ensure that the way a solution is implemented is: 1. Understood by more than one member of the team - creating a shared understanding of our platform 1. Inline with our quality standards Peer reviewing must not be left until last minute, or performed as a one-time event. Instead, we should be working closely with our colleagues to share our approach throughout the development cycle. This allows for early feedback around the way a solution is built, and allows for the reviewer to gain a greater degree of context. The final stage of peer reviewing is when code is merged into a parent branch (e.g. Work Branch --> Feature Branch, or Feature Branch --> Release Branch). This process allows for in depth reviewing of the full solution.","title":"Overview"},{"location":"6.-Engineering/Peer-Reviewing/#pair-programming-wip-reviewing","text":"Think about potential review points as a team during sprint planning! Creating discrete review tasks will help track progress and remind the team of the need to review. Pair programming, screen share walk throughs, or simply reviewing a work branch are some ways to perform peer reviews of WIP. Any member of the team can perform a peer review for anyone else, not just a more senior member!","title":"Pair Programming &amp; WIP Reviewing"},{"location":"6.-Engineering/Peer-Reviewing/#providing-feedback-for-wip","text":"Feedback should be given directly to the original engineer by the reviewer. The reviewer should feel free to ask questions and provide their honest perspective. It is wise to support feedback with rationale; in what way could the solution be improved? how will these improvements help us improve quality? The original engineer is not obliged to act upon all feedback, but an open discussion will be very beneficial.","title":"Providing Feedback for WIP"},{"location":"6.-Engineering/Peer-Reviewing/#reviews-whilst-pair-programming","text":"A review should still be performed for any code written during a pair programming session. However, it may be helpful for the two engineers to perform the review together at the end of the pairing session. E.g. the engineer that has been screensharing/writing the code should check everything in, and then the other engineer lead the review. However both engineers would be encouraged to discuss whether or not the code they have written together fulfils the guidelines.=","title":"Reviews whilst pair programming"},{"location":"6.-Engineering/Peer-Reviewing/#submitting-code-for-a-review","text":"It is best to submit code to be reviewed frequently, both to get early feedback and to ensure that the reviewing engineer can get a good picture of the changes without being overwhelmed by a huge amount of changes all at once. In other words, follow the principle of little and often! When submitting a PR the engineer should follow the Pull Request Guidelines , and should ensure to provide some context of what the code being reviewed is try to achieve and how it is structured. If refactoring existing code (in preparation for subsequent changes), the refactoring should be performed separately and submitted for review in isolation. It would otherwise be much harder for the reviewing engineer to understand how the refactoring has been done if there are also additional changes and new functionality mixed in.","title":"Submitting code for a review"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/","text":"[[ TOC ]] Purpose & Audience Purpose: Outlines the current source control techniques for our codebases, referred to as our \"branching strategy\". Audience: This is targeted at any member of the team who performs work against any of the codebases, including code changes and reviews. Source Control Code is managed and stored using Git and GitHub. Branching Strategy IMPORTANT NOTE: This strategy applies to all repositions with the exception of any shared code repositories which generate NuGet packages which are dependencies of multiple other repos/solutions. Please see the specific Branching & Versioning Shared Repositories documentation. Branch Key Purpose Naming Main Reflection of production environments. At any point you should be able to branch from Main absolutely confident that it will match what is main Release A release branch is used when EITHER of following conditions are met: 1. multiple features are being released together 2. BAT is required. release/{Release-Name] Feature Contains all the changes required for a specific feature or bug, only. Note that the term \"feature branch is used as it is common practice, but it is synonymous with a backlog item - an independent, releasable, valuable product increment. feature/{Feature-Name} OR bug/{Bug-Name} Work Contains only the work of an individual within the team. Work is only ever done against a work branch work/{Engineer Name}/[feature|bug]/{Feature Name} Strategy Basics When work is started on a feature a Feature branch is created from the Main branch. Each engineer who needs to make changed to that branch must create their own Work branch, against which they can commit all changes. Regular and small commits are recommended, and should be regularly pushed . Each commit must be accompanied by a useful description of the change made. If multiple engineers are working concurrently on a single feature, and their work is interdependent (i.e. could not be usefully tested independently), each should use their own Work branch and one Work branch should be merged into another, via a pull request (PR) . When the engineer has completed their work, and it is ready for test, a PR can be submitted to merge it into the Feature branch. The PR should be actioned by another member of the team, who should perform a peer review at this time. Changes required should be performed by the original engineer against the merging Work branch, but any other member of the team should do this in their absence. When a PR is completed and the Feature branch has been updated, a build will be automatically kicked off and a releasable package created. This can then be deployed into any available QA environment for testing or PO review. When a feature has passed testing and PO review, the Feature branch can be merged into either a Release branch, the the Main branch, by performing a PR : Release Branches: > Do this if: the feature will be released alongside other features, or is required to undergo BAT. TBC- A Test Engineer will review and assess the PR to ensure that only the correct features are included for a release A new Release branch is created by branching from the Main branch if one does not already exists. Ensure that the feature branch is up-to-date with the Main branch and regression tested before merging it into the Release branch When a merge into the Release branch is completed, a build will again be started and a releasable package will be created. This can then be deployed into any BAT environment for PO signoff and Business Acceptance Testing. A release plan should be created (or updated), including the package ID. When BAT is completed, the package can be deployed to production environments. Once the release to production environments is complete, the Release branch can be merged into Main via a PR. Main Branch > Do this if: the feature is released independently and does not require BAT. Merge to Main only occurs after the changes are successfully released into the production environments. Ensure that the feature branch is up-to-date with the Main branch and regression tested before merging it into the Release branch. A release plan should be created, including the package ID. Once the release to production environments is complete, the Feature branch can be merged into Main via a PR. Once Main has been updated, communicate the change to all Platform Development teams so that any WIP branches can be updated and regression tests can be performed. // TODO: include images of each step and overall map view Creating a Feature Branch Creating a Release Branch Merging and Pull Request Merging Upstream Upstream merges occur when changes need to pulled into a parent branch. When this is done, a Pull Request (PR) must be submitted and a peer review performed by another member of the team. You should only merge good work upstream. I.e. it should already be known to pass all quality standards and tests. Merging Downstream Downstream mergers occur when changes need to be pulled from a parent branch into a child branch. For example, a release has occurred while a feature is being worked on. In this case the master branch has changed since the feature branch was created, so the feature branch must be updated with the changes. Any work branches relating to that feature branch would also need to be updated. These changes should be merged and tested prior to attempting a merge up to the parent. Deploying Packages Tooling // TODO: Recommended source control tooling such as Source Tree/ VS Code Plugins etc // TODO: High-level GitHub overview Related Training You should have a good understanding of Git principals. Our branching strategy details the usual activities, but you may need to perform more advanced git commands occasionally. Pluralsight Course: Managing Source Code with Git This course contains a series of training videos from beginner to advanced user need. Git Documentation The reference documentation can provide quick information on commands and their purpose. and there is an advanced user book Pro Git available online as well","title":"Source Control, Versioning & Branching Strategy"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#purpose-audience","text":"Purpose: Outlines the current source control techniques for our codebases, referred to as our \"branching strategy\". Audience: This is targeted at any member of the team who performs work against any of the codebases, including code changes and reviews.","title":"Purpose &amp; Audience"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#source-control","text":"Code is managed and stored using Git and GitHub.","title":"Source Control"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#branching-strategy","text":"IMPORTANT NOTE: This strategy applies to all repositions with the exception of any shared code repositories which generate NuGet packages which are dependencies of multiple other repos/solutions. Please see the specific Branching & Versioning Shared Repositories documentation. Branch Key Purpose Naming Main Reflection of production environments. At any point you should be able to branch from Main absolutely confident that it will match what is main Release A release branch is used when EITHER of following conditions are met: 1. multiple features are being released together 2. BAT is required. release/{Release-Name] Feature Contains all the changes required for a specific feature or bug, only. Note that the term \"feature branch is used as it is common practice, but it is synonymous with a backlog item - an independent, releasable, valuable product increment. feature/{Feature-Name} OR bug/{Bug-Name} Work Contains only the work of an individual within the team. Work is only ever done against a work branch work/{Engineer Name}/[feature|bug]/{Feature Name}","title":"Branching Strategy"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#strategy-basics","text":"When work is started on a feature a Feature branch is created from the Main branch. Each engineer who needs to make changed to that branch must create their own Work branch, against which they can commit all changes. Regular and small commits are recommended, and should be regularly pushed . Each commit must be accompanied by a useful description of the change made. If multiple engineers are working concurrently on a single feature, and their work is interdependent (i.e. could not be usefully tested independently), each should use their own Work branch and one Work branch should be merged into another, via a pull request (PR) . When the engineer has completed their work, and it is ready for test, a PR can be submitted to merge it into the Feature branch. The PR should be actioned by another member of the team, who should perform a peer review at this time. Changes required should be performed by the original engineer against the merging Work branch, but any other member of the team should do this in their absence. When a PR is completed and the Feature branch has been updated, a build will be automatically kicked off and a releasable package created. This can then be deployed into any available QA environment for testing or PO review. When a feature has passed testing and PO review, the Feature branch can be merged into either a Release branch, the the Main branch, by performing a PR : Release Branches: > Do this if: the feature will be released alongside other features, or is required to undergo BAT. TBC- A Test Engineer will review and assess the PR to ensure that only the correct features are included for a release A new Release branch is created by branching from the Main branch if one does not already exists. Ensure that the feature branch is up-to-date with the Main branch and regression tested before merging it into the Release branch When a merge into the Release branch is completed, a build will again be started and a releasable package will be created. This can then be deployed into any BAT environment for PO signoff and Business Acceptance Testing. A release plan should be created (or updated), including the package ID. When BAT is completed, the package can be deployed to production environments. Once the release to production environments is complete, the Release branch can be merged into Main via a PR. Main Branch > Do this if: the feature is released independently and does not require BAT. Merge to Main only occurs after the changes are successfully released into the production environments. Ensure that the feature branch is up-to-date with the Main branch and regression tested before merging it into the Release branch. A release plan should be created, including the package ID. Once the release to production environments is complete, the Feature branch can be merged into Main via a PR. Once Main has been updated, communicate the change to all Platform Development teams so that any WIP branches can be updated and regression tests can be performed. // TODO: include images of each step and overall map view","title":"Strategy Basics"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#creating-a-feature-branch","text":"","title":"Creating a Feature Branch"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#creating-a-release-branch","text":"","title":"Creating a Release Branch"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#merging-and-pull-request","text":"","title":"Merging and Pull Request"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#merging-upstream","text":"Upstream merges occur when changes need to pulled into a parent branch. When this is done, a Pull Request (PR) must be submitted and a peer review performed by another member of the team. You should only merge good work upstream. I.e. it should already be known to pass all quality standards and tests.","title":"Merging Upstream"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#merging-downstream","text":"Downstream mergers occur when changes need to be pulled from a parent branch into a child branch. For example, a release has occurred while a feature is being worked on. In this case the master branch has changed since the feature branch was created, so the feature branch must be updated with the changes. Any work branches relating to that feature branch would also need to be updated. These changes should be merged and tested prior to attempting a merge up to the parent.","title":"Merging Downstream"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#deploying-packages","text":"","title":"Deploying Packages"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#tooling","text":"// TODO: Recommended source control tooling such as Source Tree/ VS Code Plugins etc // TODO: High-level GitHub overview","title":"Tooling"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#related-training","text":"You should have a good understanding of Git principals. Our branching strategy details the usual activities, but you may need to perform more advanced git commands occasionally.","title":"Related Training"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#pluralsight","text":"Course: Managing Source Code with Git This course contains a series of training videos from beginner to advanced user need.","title":"Pluralsight"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#git-documentation","text":"The reference documentation can provide quick information on commands and their purpose. and there is an advanced user book Pro Git available online as well","title":"Git Documentation"},{"location":"6.-Engineering/Structure-and-Patterns/","text":"Who is this for? This section is aimed at Software Engineers, but may also be of interest to other members of the team. It provides details of how our code is structured, the patterns used, and practical examples of how to implement them.","title":"Who is this for?"},{"location":"6.-Engineering/Structure-and-Patterns/#who-is-this-for","text":"This section is aimed at Software Engineers, but may also be of interest to other members of the team. It provides details of how our code is structured, the patterns used, and practical examples of how to implement them.","title":"Who is this for?"},{"location":"6.-Engineering/Test-Environments/","text":"Purpose This page documents our test environments and the process a Feature/Change follows from being developed to being released to Live. These practices will be being reviewed and updated rapidly as we work to reduce dependencies and blockers in the development, test, and deployment cycle. Guiding Principals There should be no inherent link between test environment type and development lifecycle stage. We must avoid principals such as \"QA is for testing, then it goes to stage, before it can go live\". We must ensure we perform good testing, but should not be constrained by imagined barriers. The many \"QA environments\" can be used for any type of testing, include PO The single \"Stage\" environment is used for Business Acceptance Testing (BAT) only. This is because BAT has specific test data in it at present, so the QA environments are not sufficient Turn on and off your QA environments when you start and finish using them - they cost money to keep on! Always ensure every component (i.e. each microservice) within a QA environment has the latest main branch (i.e. live) deployed to it before deploying the feature(s) under test. Test Environment Overview We currently have 6 QA environments (namely QA & QA-1 thru 5) and one Stage environment (Note: We plan to soon have 10 QA environments \u2013 QA & QA-1 thru 9). The QA environments are predominately used by the Delivery Team to test the Feature/Change The Stage environment is used by Stakeholders for BAT Only environments that are currently in use are ever switched on Deploying to Test Environments Delivery Team Testing A Feature/Change has been developed and is ready for testing - 1. Switch on a QA environment 1. Deploy the latest microservices along with the Feature/Change under test to the QA environment 1. Test the Feature/Change 1. Any defects raised at this phase are fixed and released to same QA environment * On release, ensure the latest microservices are deployed 1. On completion of this test phase, check all deployed microservices are the same version as Live * If the microservices remain the latest, the Feature/Change can move to the next phase i.e. PO Sign-Off * If the microservices no longer match Live: * Deploy the latest versions to the QA environment * Smoke Test * Move to the next phase i.e. PO Sign-Off For further information on the release process, please see Deploying to Test Environments PO Sign-Off PO Sign-Off takes place on the QA environment with the latest versions of the microservices deployed (i.e. as close to Live as possible) Ideally, PO will be able to sign-off the release promptly to prevent further smoke testing should the microservices fall out of sync with Live. Any defects raised at this phase are fixed and released to same QA environment On release, ensure the latest microservices are deployed Once PO Sign-Off is obtained, the next phase depends on whether BAT is required or not BAT Phase Not Required Testing is complete and PO have signed off the Feature/Change against the latest microservices. The Feature/Change is now ready to be released to Demo & Live. BAT Phase Required Testing is complete and PO have signed off the Feature/Change against the latest microservices. The Feature/Change is now ready to be released to Stage for BAT. 1. Check no BAT session is currently taking place on the Stage environment 1. Switch on the Stage environment 1. Deploy the latest microservices along with the Feature/Change under test to the Stage environment 1. Smoke test where required 1. BAT can commence Following sign-off from Stakeholders, a smoke test may be required should the microservices no longer be up to date (i.e. release to Live occurred during this time). The Feature/Change is now ready to be released to Demo & Live. Following Release to Live On completion of the release: 1. Follow the branching strategy guidelines to ensure the main branch is updated 1. The relevant QA environment and Stage (if not being used by another BAT session) must be switched off WorkFlow Diagram ::: mermaid graph TD A[Development Complete] --> |Switch on QA Environment| B[Deploy latest microservices] B --> C[Deploy Code Changes] C --> D{Defects Found} D --> |Yes| A D --> |No| E{Latest microservices still deployed?} E --> |No| F[Deploy latest Microservices] F --> G[Smoke Test] G --> H[PO Sign Off] E --> |Yes| H H --> I{Defects Found} I --> |Yes| A I --> |No| J{Latest microservices still deployed?} J --> |No| K[Deploy latest Microservices] K --> L[Smoke Test] L --> M{BAT Required?} J --> |Yes| M M --> |No| N[Release to Demo & Live] M --> |Yes| O[Switch on Stage Environment] O --> P[Deploy latest microservices] P --> Q[Deploy Code Changes] Q --> R[BAT Signed Off] R --> |Yes| S{Latest microservices still deployed?} S --> |No| T[Deploy latest Microservices] T --> U[Smoke Test] U --> N S --> |Yes| N :::","title":"Purpose"},{"location":"6.-Engineering/Test-Environments/#purpose","text":"This page documents our test environments and the process a Feature/Change follows from being developed to being released to Live. These practices will be being reviewed and updated rapidly as we work to reduce dependencies and blockers in the development, test, and deployment cycle.","title":"Purpose"},{"location":"6.-Engineering/Test-Environments/#guiding-principals","text":"There should be no inherent link between test environment type and development lifecycle stage. We must avoid principals such as \"QA is for testing, then it goes to stage, before it can go live\". We must ensure we perform good testing, but should not be constrained by imagined barriers. The many \"QA environments\" can be used for any type of testing, include PO The single \"Stage\" environment is used for Business Acceptance Testing (BAT) only. This is because BAT has specific test data in it at present, so the QA environments are not sufficient Turn on and off your QA environments when you start and finish using them - they cost money to keep on! Always ensure every component (i.e. each microservice) within a QA environment has the latest main branch (i.e. live) deployed to it before deploying the feature(s) under test.","title":"Guiding Principals"},{"location":"6.-Engineering/Test-Environments/#test-environment-overview","text":"We currently have 6 QA environments (namely QA & QA-1 thru 5) and one Stage environment (Note: We plan to soon have 10 QA environments \u2013 QA & QA-1 thru 9). The QA environments are predominately used by the Delivery Team to test the Feature/Change The Stage environment is used by Stakeholders for BAT Only environments that are currently in use are ever switched on","title":"Test Environment Overview"},{"location":"6.-Engineering/Test-Environments/#deploying-to-test-environments","text":"","title":"Deploying to Test Environments"},{"location":"6.-Engineering/Test-Environments/#delivery-team-testing","text":"A Feature/Change has been developed and is ready for testing - 1. Switch on a QA environment 1. Deploy the latest microservices along with the Feature/Change under test to the QA environment 1. Test the Feature/Change 1. Any defects raised at this phase are fixed and released to same QA environment * On release, ensure the latest microservices are deployed 1. On completion of this test phase, check all deployed microservices are the same version as Live * If the microservices remain the latest, the Feature/Change can move to the next phase i.e. PO Sign-Off * If the microservices no longer match Live: * Deploy the latest versions to the QA environment * Smoke Test * Move to the next phase i.e. PO Sign-Off For further information on the release process, please see Deploying to Test Environments","title":"Delivery Team Testing"},{"location":"6.-Engineering/Test-Environments/#po-sign-off","text":"PO Sign-Off takes place on the QA environment with the latest versions of the microservices deployed (i.e. as close to Live as possible) Ideally, PO will be able to sign-off the release promptly to prevent further smoke testing should the microservices fall out of sync with Live. Any defects raised at this phase are fixed and released to same QA environment On release, ensure the latest microservices are deployed Once PO Sign-Off is obtained, the next phase depends on whether BAT is required or not","title":"PO Sign-Off"},{"location":"6.-Engineering/Test-Environments/#bat-phase-not-required","text":"Testing is complete and PO have signed off the Feature/Change against the latest microservices. The Feature/Change is now ready to be released to Demo & Live.","title":"BAT Phase Not Required"},{"location":"6.-Engineering/Test-Environments/#bat-phase-required","text":"Testing is complete and PO have signed off the Feature/Change against the latest microservices. The Feature/Change is now ready to be released to Stage for BAT. 1. Check no BAT session is currently taking place on the Stage environment 1. Switch on the Stage environment 1. Deploy the latest microservices along with the Feature/Change under test to the Stage environment 1. Smoke test where required 1. BAT can commence Following sign-off from Stakeholders, a smoke test may be required should the microservices no longer be up to date (i.e. release to Live occurred during this time). The Feature/Change is now ready to be released to Demo & Live.","title":"BAT Phase Required"},{"location":"6.-Engineering/Test-Environments/#following-release-to-live","text":"On completion of the release: 1. Follow the branching strategy guidelines to ensure the main branch is updated 1. The relevant QA environment and Stage (if not being used by another BAT session) must be switched off","title":"Following Release to Live"},{"location":"6.-Engineering/Test-Environments/#workflow-diagram","text":"::: mermaid graph TD A[Development Complete] --> |Switch on QA Environment| B[Deploy latest microservices] B --> C[Deploy Code Changes] C --> D{Defects Found} D --> |Yes| A D --> |No| E{Latest microservices still deployed?} E --> |No| F[Deploy latest Microservices] F --> G[Smoke Test] G --> H[PO Sign Off] E --> |Yes| H H --> I{Defects Found} I --> |Yes| A I --> |No| J{Latest microservices still deployed?} J --> |No| K[Deploy latest Microservices] K --> L[Smoke Test] L --> M{BAT Required?} J --> |Yes| M M --> |No| N[Release to Demo & Live] M --> |Yes| O[Switch on Stage Environment] O --> P[Deploy latest microservices] P --> Q[Deploy Code Changes] Q --> R[BAT Signed Off] R --> |Yes| S{Latest microservices still deployed?} S --> |No| T[Deploy latest Microservices] T --> U[Smoke Test] U --> N S --> |Yes| N :::","title":"WorkFlow Diagram"},{"location":"6.-Engineering/Peer-Reviewing/Performing-a-Peer-Review/","text":"The engineer performing the review must: Copy and paste the template checklist (below) into their a new comment on the PR in GitHub Start reviewing the changes with respect to the Quality Standards When the reviewer is happy that a particular guideline has been met, the relevant item in the checklist is checked If any guidelines are not met, the reviewer should add a comment (either a general comment, or next to the offending portion of code) to explain why When the reviewer has finished reviewing the code, they must submit the review via GitHub: * If all the guidelines have all been met, they should tick the Approve option * Otherwise, they should tick the Request Changes option Engineers are also encouraged to provide positive feedback. If code being submitted for review is well-structured and meets all guidelines, then this should be celebrated. Checklist Template - [ ] Code implements the backlog item - [ ] Code is readable and well-structured - [ ] Code adheres to SOLID design principles - [ ] Code is not duplicated, or is moved into shared packages where relevant - [ ] Endpoint paths are resful and follow guidelines - [ ] Avoid hardcoded variables - all behaviour should be configurable via configuration settings - [ ] Code has good logging throughout - [ ] Code is well unit-tested - [ ] Minimum coverage level of approx 80% is maintained - [ ] Code coverage has been increased where it is not already at approx 80% - [ ] New UI elements are Accessible - [ ] Code follows Security best-practices - [ ] Code follows Performance best-practices - [ ] Documentation has been created or updated","title":"Performing a Peer Review"},{"location":"6.-Engineering/Peer-Reviewing/Performing-a-Peer-Review/#checklist-template","text":"- [ ] Code implements the backlog item - [ ] Code is readable and well-structured - [ ] Code adheres to SOLID design principles - [ ] Code is not duplicated, or is moved into shared packages where relevant - [ ] Endpoint paths are resful and follow guidelines - [ ] Avoid hardcoded variables - all behaviour should be configurable via configuration settings - [ ] Code has good logging throughout - [ ] Code is well unit-tested - [ ] Minimum coverage level of approx 80% is maintained - [ ] Code coverage has been increased where it is not already at approx 80% - [ ] New UI elements are Accessible - [ ] Code follows Security best-practices - [ ] Code follows Performance best-practices - [ ] Documentation has been created or updated","title":"Checklist Template"},{"location":"6.-Engineering/Peer-Reviewing/Pull-Requests/","text":"Pull Requests A pull request (PR) is required when merging a child branch into a parent . NOTE: manually merging or committing directly to a parent branch is not permitted. Firstly because it skips the PR mechanism (and therefore bypasses an important quality control step), and secondly because only a PR will trigger the relevant Continuous Integration workflows and builds. A pull request will ideally not be the first time another engineer has seen the work in progress! Think of it more as a final check; pair programming and ad-hoc discussions on possible approaches and patterns would ideally have already given one or more other engineers in the team an opportunity to see the code and provide feedback. Remember, early feedback means we can spot and rectify mistakes earlier. Doing so later in the process will require more time and effort. Creating a Pull Request When an engineer has completed their work on a child branch, and has committed and pushed it to GitHub, they should visit the GitHub website and issue a pull request. This can be done either on the repository's main page (GitHub will usually prompt a user to issue a pull request if they have recently pushed a child branch), or by navigating to the Pull requests tab and clicking the green New pull request button. When creating the pull request, the engineer submitting their work should make sure to: Ensure the base branch is correct (this should be the parent branch, e.g. a Feature branch when merging in a Work branch when working on a new feature) Ensure the compare branch is correct (this should be the engineer's work branch in most situations) Populate the title with a concise summary of what is being submitted Populate the body with an explanation of their approach to solving the problem, including any patterns used Link the request to the relevant ticket in Azure DevOps by adding AB#123 where 123 is the ID of the ticket If there are PRs in other repos that are related to the same piece of work, link to them in the PR description (see hints, below) This shouldn't need to be too detailed, as ideally this won't be the first time they are seeing the code In the case of a shared code repository, provided the information about version number update (i.e. MAJOR vs MINOR vs PATCH) and why it is appropriate Assign their team as a reviewer NOTE: the final step will automatically assign all other team members, and only one team member needs to approve the PR. The engineer submitting the PR should communicate with the rest of their team to request a review. Reviewing Pull Requests & Providing Feedback The reviewing engineer(s) should get a GitHub notification, and should review the PR at the earliest opportunity, and discuss any questions they may have or suggested changes/fixes that need to be made with the original engineer. Once the review has been completed and the reviewing engineer is happy with the changes (including any agreed changes having been completed and committed), they should approve the changes. The original engineer is then responsible for merging the changes into the parent branch, and monitoring any CI workflows to ensure they complete successfully. Feedback should be giving via comments in the pull request in GitHub. Any member of the team can perform a peer review for anyone else, not just a more senior member! Hints for working with Pull Requests in GitHub You can open a draft Pull Request even before the work is ready for review. When you have finished working on it, you can then click the Ready for Review button on the PR page. Any further commits pushed to the same branch before the Pull Request is merged will be included. This works for both draft and non-draft PRs Beware , the default behaviour is that GitHub will create a PR that merges into the main branch, which is rarely correct. However, it is possible to change the base of a PR before it is merged, simply by clicking the Edit button at the top of the PR page, and then selecting the relevant feature or release branch from the drop down. If you add a link to another GitHub Pull Request (usually from another repo) in the body of your pull request, or within another comment, GitHub will display the status of the linked PR with the main one. Hovering over the link will show a \"card\" containing more details of that other PR.","title":"Pull Requests"},{"location":"6.-Engineering/Peer-Reviewing/Pull-Requests/#pull-requests","text":"A pull request (PR) is required when merging a child branch into a parent . NOTE: manually merging or committing directly to a parent branch is not permitted. Firstly because it skips the PR mechanism (and therefore bypasses an important quality control step), and secondly because only a PR will trigger the relevant Continuous Integration workflows and builds. A pull request will ideally not be the first time another engineer has seen the work in progress! Think of it more as a final check; pair programming and ad-hoc discussions on possible approaches and patterns would ideally have already given one or more other engineers in the team an opportunity to see the code and provide feedback. Remember, early feedback means we can spot and rectify mistakes earlier. Doing so later in the process will require more time and effort.","title":"Pull Requests"},{"location":"6.-Engineering/Peer-Reviewing/Pull-Requests/#creating-a-pull-request","text":"When an engineer has completed their work on a child branch, and has committed and pushed it to GitHub, they should visit the GitHub website and issue a pull request. This can be done either on the repository's main page (GitHub will usually prompt a user to issue a pull request if they have recently pushed a child branch), or by navigating to the Pull requests tab and clicking the green New pull request button. When creating the pull request, the engineer submitting their work should make sure to: Ensure the base branch is correct (this should be the parent branch, e.g. a Feature branch when merging in a Work branch when working on a new feature) Ensure the compare branch is correct (this should be the engineer's work branch in most situations) Populate the title with a concise summary of what is being submitted Populate the body with an explanation of their approach to solving the problem, including any patterns used Link the request to the relevant ticket in Azure DevOps by adding AB#123 where 123 is the ID of the ticket If there are PRs in other repos that are related to the same piece of work, link to them in the PR description (see hints, below) This shouldn't need to be too detailed, as ideally this won't be the first time they are seeing the code In the case of a shared code repository, provided the information about version number update (i.e. MAJOR vs MINOR vs PATCH) and why it is appropriate Assign their team as a reviewer NOTE: the final step will automatically assign all other team members, and only one team member needs to approve the PR. The engineer submitting the PR should communicate with the rest of their team to request a review.","title":"Creating a Pull Request"},{"location":"6.-Engineering/Peer-Reviewing/Pull-Requests/#reviewing-pull-requests-providing-feedback","text":"The reviewing engineer(s) should get a GitHub notification, and should review the PR at the earliest opportunity, and discuss any questions they may have or suggested changes/fixes that need to be made with the original engineer. Once the review has been completed and the reviewing engineer is happy with the changes (including any agreed changes having been completed and committed), they should approve the changes. The original engineer is then responsible for merging the changes into the parent branch, and monitoring any CI workflows to ensure they complete successfully. Feedback should be giving via comments in the pull request in GitHub. Any member of the team can perform a peer review for anyone else, not just a more senior member!","title":"Reviewing Pull Requests &amp; Providing Feedback"},{"location":"6.-Engineering/Peer-Reviewing/Pull-Requests/#hints-for-working-with-pull-requests-in-github","text":"You can open a draft Pull Request even before the work is ready for review. When you have finished working on it, you can then click the Ready for Review button on the PR page. Any further commits pushed to the same branch before the Pull Request is merged will be included. This works for both draft and non-draft PRs Beware , the default behaviour is that GitHub will create a PR that merges into the main branch, which is rarely correct. However, it is possible to change the base of a PR before it is merged, simply by clicking the Edit button at the top of the PR page, and then selecting the relevant feature or release branch from the drop down. If you add a link to another GitHub Pull Request (usually from another repo) in the body of your pull request, or within another comment, GitHub will display the status of the linked PR with the main one. Hovering over the link will show a \"card\" containing more details of that other PR.","title":"Hints for working with Pull Requests in GitHub"},{"location":"6.-Engineering/Quality-Standards/","text":"[[ TOC ]] Who is this for? Outlines the standards to which Software Engineers and Test Engineers must adhere when writing code. Overview Quality means more than simply the way code is structured! Great quality solutions cover many facets, including: Readability & Maintainability Security & Compliance Robustness & Performance Architectural integrity Cost & environmentally friendly This is why it is important to share our approach to solutions early! Peer reviews should be looking to improve any/all of these areas, and it may often be too late by the time you have working code ready to merge. Guiding Principals Code must be easy to read and understand C# Code must follow OO patterns and SOLID principals Coding and naming conventions must be followed Unit and integrations test coverage must be maintained or improved Event logging must be used to provide useful diagnostic information Code must be self describing, and must not rely on code commenting to demystify chaos Code must be well documented, and must use code commenting and READMEs to enable rapid discovery User interfaces must be accessible and conform to WCAG AA Code must be secure and follow industry recommended practices Code must be performant, and able to handle live-like datasets and load Code should be green Guidelines for high-quality code Readable and Maintainable First and foremost, high-quality code must be easy to read and understand. Code must also follow our in-house coding conventions . The names of methods and classes etc. must clearly indicate what they do, and methods and classes must not be too lengthy. Code must also follow the SOLID principles , which will help to make it better structured, more readable and keep methods and classes manageable sizes. When creating new endpoints on microservices, ensure that they follow the endpoint naming conventions . Making code more maintainable is effectively the practice of minimising the amount of times you have to update it. Avoid using hardcoded values in code, as these values can only be changed by changing the code. Instead, consider making the behaviour of the code configurable by offloading these values to configuration. Similarly, do not repeat yourself (DRY)! Avoid copy/pasting code; instead consider how and where to share it. For smaller snippets of re-usable code, consider putting it in a helper class or extension method. Also consider the scope of any shared code; is it specific to the current repository, or is it more general code that might benefit from living in a shared code library for re-use across other repositories too? Following the Single Responsibility Principle (one of the SOLID principles, see above) should help with this. Do not commit commented-out code. It makes code less readable, and also causes confusion. When commented-out code is committed to a repository, it is not clear whether it should simply be deleted, or the code needs to be adapted and integrated, i.e. is it a cryptic TODO? Unit & Integration Tests Code must be well covered by unit & integration tests . Unit & integration testing not only increases our chances of catching bugs, increases engineer confidence in any changes being made, but is also an indication that the code is well-structured. To meet quality guidelines, all .NET code must be covered by Unit and/or integration tests with the aim of achieving 80% code coverage. For legacy repositories that do not meet this threshold, the aim should be to increase the coverage coverage percentage when writing new code, so as to incrementally meet the desired threshold. For repositories that are already well-tested, all new code must be sufficiently well tested so as to not bring the average coverage down. NOTE: code coverage is not a perfect metric. It is entirely possible to write poor tests that achieve an arbitrary coverage without providing any of the real benefits. Code repositories must measure and report on code coverage metrics as part of Continuous Integration. If not already present, add the code coverage testing to the repository being worked on. As engineers write tests for a repository, they must ensure the tests.yml file is updated with the latest coverage level. Observability & Logging Building observable systems enables development engineers to measure how well the application is behaving. Observability serves the following goals: Provide holistic view of the application health. Help measure business performance for the customer. Measure operational performance of the system. Identify and diagnose failures to get to the problem fast. A critical part of this is ensuring that services are logging useful events and errors. Log messages need to contain enough information to help an engineer understand what was happening, including for example Website Key or Client Key, ID of an asset or document affected, action being performed etc. See more information about logging in Quartex , and further reading on general best practices for logging. Documentation Every software development project requires documentation. Agile Software Development values working software over comprehensive documentation . However, repositories must include the key information needed to understand the development and the use of the generated software. Good documentation should work towards these goals: Facilitate the onboarding of new team members Improve communication and collaboration between teams Enable other engineers to successfully work with the software Every repository must have a README, which should succinctly explain: The purpose of the repository For a service, this boils down to \"what is the service responsible for?\" Otherwise explain the use-cases for using or modifying the repository Any external dependencies that are not automatically imported Additional steps required to build or debug the code Any processes that are unique to this repository Further reading on documentation guidelines . Documentation must never include any passwords or other sensitive configuration. These should be stored and documented in a secure manner, such as in a Password Manager ( TODO - setup password management for division ) or in a Secrets Manager. Any hacks or shortcuts or TODOs must be documented, as these all represent the addition of different types of technical debt. Engineers must be mindful of any technical debt being committed, and must have a plan to resolve it. This might include creating a backlog item to address the technical debt, with a plan to bring it into a subsequent sprint. Accessibility Any new UI elements must be made accessible, and conform to WCAG AA. This includes making the UI usable via Screen Reader, including adding aria tags where relevant. The UI must also be usable with the keyboard. Secure Code Engineers must ensure that code adheres to industry-recommended standard practices for secure design and implementation of code. For practical purposes, engineers must be familiar with the OWASP top 10 vulnerabilities . The OWASP secure coding practices guide is also a useful reference. Ensuring software is secure relies not just on the software being written, but also the Frameworks and Libraries it depends on; which makes keeping these updated is important. Secure software also needs to be run on infrastructure, which must itself be up-to-date and configured securely. When writing and committing code, engineers must ensure they are never commit secrets (i.e database passwords, access keys or other sensitive config) into source control or documentation. New endpoints and pages must have the correct authentication and authorisation checks in place. Performant Code Engineers must ensure the code they write is performant. This refers both to ensuring individual actions and pages are loaded within an acceptable timeframe, and ensuring that functionality does not degrade with large datasets. In particular it is important to perform testing with live-like datasets to ensure the software will behave as expected. NOTE: specific non-functional requirements for acceptable performance will be defined at a later date. The Microsoft article on .NET Core performance best practices is very helpful. General performance tips Use caching where ever it is possible and appropriate. The Quartex caching guidelines contain mechanisms for a range of different types of caching, including the caching service calls for retrieving data, and output caching to cache entire pages. The fastest way of doing work is to not have to do the work at all! Use async / await calls wherever possible. The async pattern allows the operatic system to assign idle CPU capacity to a thread that has work to do, instead of keeping the thread busy. Whilst it may be difficult to see much of a difference when developing locally, using async / await allows a webserver to handle many more requests in parallel. Entity Framework tips When retrieving data from databases, use .Include() calls only where strictly necessary. Whilst it might be quite convenient, it is likely that with a large dataset, the increased amount of data returned will have a performance impact which may not be apparent when developing locally. This simply further re-enforces the need to test with live-like datasets. Avoid using queries that Entity Framework will not be able to convert to SQL. A common mistake is to use .ToLowerInvariant() in a .Where() clause to perform a case-insensitive search. In these scenarios, Entity Framework will translate as much as possible into SQL, but then retrieve the rest of the data set and perform the rest of the query in memory - potentially resulting in a huge dataset being retrieved from the database. E.g. consider the following fictitious query which retrieves an asset that matches a given path, where deleted is false. var children = _unitOfWork.Assets.Where(a => a.Deleted == false && a.Path.ToLowerInvariant() == pathToSearchFor); In this example, since EF is unable to translate .ToLowerInvariant() into SQL, it will effectively run a query like select * from Assets where Deleted=0 and then run the Path.ToLowerInvariant() == pathToSearchFor comparison in memory, on every single returned row. For a very large database, this sort of subtlety can be cripplingly slow!","title":"Index"},{"location":"6.-Engineering/Quality-Standards/#who-is-this-for","text":"Outlines the standards to which Software Engineers and Test Engineers must adhere when writing code.","title":"Who is this for?"},{"location":"6.-Engineering/Quality-Standards/#overview","text":"Quality means more than simply the way code is structured! Great quality solutions cover many facets, including: Readability & Maintainability Security & Compliance Robustness & Performance Architectural integrity Cost & environmentally friendly This is why it is important to share our approach to solutions early! Peer reviews should be looking to improve any/all of these areas, and it may often be too late by the time you have working code ready to merge.","title":"Overview"},{"location":"6.-Engineering/Quality-Standards/#guiding-principals","text":"Code must be easy to read and understand C# Code must follow OO patterns and SOLID principals Coding and naming conventions must be followed Unit and integrations test coverage must be maintained or improved Event logging must be used to provide useful diagnostic information Code must be self describing, and must not rely on code commenting to demystify chaos Code must be well documented, and must use code commenting and READMEs to enable rapid discovery User interfaces must be accessible and conform to WCAG AA Code must be secure and follow industry recommended practices Code must be performant, and able to handle live-like datasets and load Code should be green","title":"Guiding Principals"},{"location":"6.-Engineering/Quality-Standards/#guidelines-for-high-quality-code","text":"","title":"Guidelines for high-quality code"},{"location":"6.-Engineering/Quality-Standards/#readable-and-maintainable","text":"First and foremost, high-quality code must be easy to read and understand. Code must also follow our in-house coding conventions . The names of methods and classes etc. must clearly indicate what they do, and methods and classes must not be too lengthy. Code must also follow the SOLID principles , which will help to make it better structured, more readable and keep methods and classes manageable sizes. When creating new endpoints on microservices, ensure that they follow the endpoint naming conventions . Making code more maintainable is effectively the practice of minimising the amount of times you have to update it. Avoid using hardcoded values in code, as these values can only be changed by changing the code. Instead, consider making the behaviour of the code configurable by offloading these values to configuration. Similarly, do not repeat yourself (DRY)! Avoid copy/pasting code; instead consider how and where to share it. For smaller snippets of re-usable code, consider putting it in a helper class or extension method. Also consider the scope of any shared code; is it specific to the current repository, or is it more general code that might benefit from living in a shared code library for re-use across other repositories too? Following the Single Responsibility Principle (one of the SOLID principles, see above) should help with this. Do not commit commented-out code. It makes code less readable, and also causes confusion. When commented-out code is committed to a repository, it is not clear whether it should simply be deleted, or the code needs to be adapted and integrated, i.e. is it a cryptic TODO?","title":"Readable and Maintainable"},{"location":"6.-Engineering/Quality-Standards/#unit-integration-tests","text":"Code must be well covered by unit & integration tests . Unit & integration testing not only increases our chances of catching bugs, increases engineer confidence in any changes being made, but is also an indication that the code is well-structured. To meet quality guidelines, all .NET code must be covered by Unit and/or integration tests with the aim of achieving 80% code coverage. For legacy repositories that do not meet this threshold, the aim should be to increase the coverage coverage percentage when writing new code, so as to incrementally meet the desired threshold. For repositories that are already well-tested, all new code must be sufficiently well tested so as to not bring the average coverage down. NOTE: code coverage is not a perfect metric. It is entirely possible to write poor tests that achieve an arbitrary coverage without providing any of the real benefits. Code repositories must measure and report on code coverage metrics as part of Continuous Integration. If not already present, add the code coverage testing to the repository being worked on. As engineers write tests for a repository, they must ensure the tests.yml file is updated with the latest coverage level.","title":"Unit &amp; Integration Tests"},{"location":"6.-Engineering/Quality-Standards/#observability-logging","text":"Building observable systems enables development engineers to measure how well the application is behaving. Observability serves the following goals: Provide holistic view of the application health. Help measure business performance for the customer. Measure operational performance of the system. Identify and diagnose failures to get to the problem fast. A critical part of this is ensuring that services are logging useful events and errors. Log messages need to contain enough information to help an engineer understand what was happening, including for example Website Key or Client Key, ID of an asset or document affected, action being performed etc. See more information about logging in Quartex , and further reading on general best practices for logging.","title":"Observability &amp; Logging"},{"location":"6.-Engineering/Quality-Standards/#documentation","text":"Every software development project requires documentation. Agile Software Development values working software over comprehensive documentation . However, repositories must include the key information needed to understand the development and the use of the generated software. Good documentation should work towards these goals: Facilitate the onboarding of new team members Improve communication and collaboration between teams Enable other engineers to successfully work with the software Every repository must have a README, which should succinctly explain: The purpose of the repository For a service, this boils down to \"what is the service responsible for?\" Otherwise explain the use-cases for using or modifying the repository Any external dependencies that are not automatically imported Additional steps required to build or debug the code Any processes that are unique to this repository Further reading on documentation guidelines . Documentation must never include any passwords or other sensitive configuration. These should be stored and documented in a secure manner, such as in a Password Manager ( TODO - setup password management for division ) or in a Secrets Manager. Any hacks or shortcuts or TODOs must be documented, as these all represent the addition of different types of technical debt. Engineers must be mindful of any technical debt being committed, and must have a plan to resolve it. This might include creating a backlog item to address the technical debt, with a plan to bring it into a subsequent sprint.","title":"Documentation"},{"location":"6.-Engineering/Quality-Standards/#accessibility","text":"Any new UI elements must be made accessible, and conform to WCAG AA. This includes making the UI usable via Screen Reader, including adding aria tags where relevant. The UI must also be usable with the keyboard.","title":"Accessibility"},{"location":"6.-Engineering/Quality-Standards/#secure-code","text":"Engineers must ensure that code adheres to industry-recommended standard practices for secure design and implementation of code. For practical purposes, engineers must be familiar with the OWASP top 10 vulnerabilities . The OWASP secure coding practices guide is also a useful reference. Ensuring software is secure relies not just on the software being written, but also the Frameworks and Libraries it depends on; which makes keeping these updated is important. Secure software also needs to be run on infrastructure, which must itself be up-to-date and configured securely. When writing and committing code, engineers must ensure they are never commit secrets (i.e database passwords, access keys or other sensitive config) into source control or documentation. New endpoints and pages must have the correct authentication and authorisation checks in place.","title":"Secure Code"},{"location":"6.-Engineering/Quality-Standards/#performant-code","text":"Engineers must ensure the code they write is performant. This refers both to ensuring individual actions and pages are loaded within an acceptable timeframe, and ensuring that functionality does not degrade with large datasets. In particular it is important to perform testing with live-like datasets to ensure the software will behave as expected. NOTE: specific non-functional requirements for acceptable performance will be defined at a later date. The Microsoft article on .NET Core performance best practices is very helpful.","title":"Performant Code"},{"location":"6.-Engineering/Quality-Standards/#general-performance-tips","text":"Use caching where ever it is possible and appropriate. The Quartex caching guidelines contain mechanisms for a range of different types of caching, including the caching service calls for retrieving data, and output caching to cache entire pages. The fastest way of doing work is to not have to do the work at all! Use async / await calls wherever possible. The async pattern allows the operatic system to assign idle CPU capacity to a thread that has work to do, instead of keeping the thread busy. Whilst it may be difficult to see much of a difference when developing locally, using async / await allows a webserver to handle many more requests in parallel.","title":"General performance tips"},{"location":"6.-Engineering/Quality-Standards/#entity-framework-tips","text":"When retrieving data from databases, use .Include() calls only where strictly necessary. Whilst it might be quite convenient, it is likely that with a large dataset, the increased amount of data returned will have a performance impact which may not be apparent when developing locally. This simply further re-enforces the need to test with live-like datasets. Avoid using queries that Entity Framework will not be able to convert to SQL. A common mistake is to use .ToLowerInvariant() in a .Where() clause to perform a case-insensitive search. In these scenarios, Entity Framework will translate as much as possible into SQL, but then retrieve the rest of the data set and perform the rest of the query in memory - potentially resulting in a huge dataset being retrieved from the database. E.g. consider the following fictitious query which retrieves an asset that matches a given path, where deleted is false. var children = _unitOfWork.Assets.Where(a => a.Deleted == false && a.Path.ToLowerInvariant() == pathToSearchFor); In this example, since EF is unable to translate .ToLowerInvariant() into SQL, it will effectively run a query like select * from Assets where Deleted=0 and then run the Path.ToLowerInvariant() == pathToSearchFor comparison in memory, on every single returned row. For a very large database, this sort of subtlety can be cripplingly slow!","title":"Entity Framework tips"},{"location":"6.-Engineering/Quality-Standards/Coding-Conventions/","text":"[[ TOC ]] C# Coding Conventions Our coding standards use the Microsoft C# conventions . All engineers must use and be familiar with them. Additional in-house conventions Controllers (and other code entry points like Background Tasks and Message Queue Message Handlers which are similar to controllers) should be kept as lean as possible. Each controller should dependency inject a Service that is responsible for business logic. - The controller should do some basic input validation (returning an appropriate error code if validation fails) and then call the service to perform the bulk of the work. - The main service can then inject as many other classes as it needs to perform its function. Smaller repeated snippets of code should be refactored out and made re-usable, using one of the following approaches: - Simpler pieces of code can be grouped into static methods within a SomethingHelper class - Extension methods should generally be grouped together based on the class they extend, and be called MyClassExtensions - Consider whether any refactored code could be useful moved into a shared package for re-use in other repositories For constructing a small string, use String Interpolation (i.e. string formatted = $\"My variable is {variable}\" ) rather than string.Format() . However if you need to log the value of a string, remember to use Serilog logging properties. For constructing a larger string e.g. within a loop, use StringBuilder instead of concatenating many strings together. Unit and Integration tests should make use Shouldly instead of Assert.That to make assertions more readable. TypeScript and JavaScript Coding Conventions TODO: document this section and link to an industry-standard style guide","title":"Coding Conventions"},{"location":"6.-Engineering/Quality-Standards/Coding-Conventions/#c-coding-conventions","text":"Our coding standards use the Microsoft C# conventions . All engineers must use and be familiar with them.","title":"C# Coding Conventions"},{"location":"6.-Engineering/Quality-Standards/Coding-Conventions/#additional-in-house-conventions","text":"Controllers (and other code entry points like Background Tasks and Message Queue Message Handlers which are similar to controllers) should be kept as lean as possible. Each controller should dependency inject a Service that is responsible for business logic. - The controller should do some basic input validation (returning an appropriate error code if validation fails) and then call the service to perform the bulk of the work. - The main service can then inject as many other classes as it needs to perform its function. Smaller repeated snippets of code should be refactored out and made re-usable, using one of the following approaches: - Simpler pieces of code can be grouped into static methods within a SomethingHelper class - Extension methods should generally be grouped together based on the class they extend, and be called MyClassExtensions - Consider whether any refactored code could be useful moved into a shared package for re-use in other repositories For constructing a small string, use String Interpolation (i.e. string formatted = $\"My variable is {variable}\" ) rather than string.Format() . However if you need to log the value of a string, remember to use Serilog logging properties. For constructing a larger string e.g. within a loop, use StringBuilder instead of concatenating many strings together. Unit and Integration tests should make use Shouldly instead of Assert.That to make assertions more readable.","title":"Additional in-house conventions"},{"location":"6.-Engineering/Quality-Standards/Coding-Conventions/#typescript-and-javascript-coding-conventions","text":"TODO: document this section and link to an industry-standard style guide","title":"TypeScript and JavaScript Coding Conventions"},{"location":"6.-Engineering/Quality-Standards/Endpoint-Naming-Conventions/","text":"The following guidelines are adapted from the REST API Tutorial . Endpoint path conventions All our microservice endpoints should start with a version (e.g. v1 , v2 etc.), which will have different uses depending on the context. Most microservice endpoints will also relate directly either to a specific Client or specific Website ; although some microservices don't require this (e.g. the File Validation service): /{version}/{ClientKey}/rest/of/url - Version and ClientKey /{version}/{WebsiteKey}/rest/of/url - Version and WebsiteKey /{version}/rest/of/url - Version only Requests paths should use plural nouns, and indicate the hierarchy of the objects that they represent as much as possible. It can be helpful to think of every endpoint path as representing a specific object (or a specific set of objects). /{version}/{client}/jobs - Get all Jobs /{version}/{client}/jobs/{JobId} - Get details for a single Job /{version}/{client}/jobs/{JobId}/items - Get all the Items for a single Job /{version}/{client}/jobs/{JobId}/items/{ItemId} - Get details for a single Item within a given Job NOTE : avoid using verbs in endpoint paths whereever possible. E.g. don't use getasset/{assetId} because the get is already in the HTTP verb. This leads nicely onto the following section... Use the relevant HTTP Verb for each action. Use GET requests for reading data POST requests should be creating new items or uploading files PUT requests should be used for updating existing items DELETE requests should be used for deleting data If we consider each endpoint's path to be a specific object, then it makes sense that each HTTP verb is performing the relevant action on that object. E.g. a GET request will retrieve the object, a PUT request will modify the same object, and a DELETE request would delete that same object. For GET requests, only identifiers (e.g. numerical IDs or Paths) should go in to the URL; filters, sorting or pagination (i.e. skip and take ) parameters should be included as Query String values (implement the IQueryStringRequest interface). In extreme cases it might be necessary to use a POST request to include all the required parameters but avoid doing this if at all possible as it strictly speaking the wrong verb. Some examples below: /{version}/{client}/assets/{skip}/{take} - instead use /{version}/{client}/assets?skip={skip}&take={take} In principle, there could be several identical looking URLs - but using different HTTP Verbs would perform different actions. E.g. GET /{version}/{client}/jobs - Get all jobs. This may include pagination or filtering as query string GET /{version}/{client}/jobs/{JobId} - Get details for a specific job PUT /{version}/{client}/jobs/{JobId} - Update a job (e.g. set its status to Complete - the exact properties to be updated would live in the request body) DELETE /{version}/{client}/jobs/{JobId} - Delete the job Wherever possible avoid putting an action into the URL if it can be indicated by the HTTP Verb. The only exception to this is where there are multiple similar actions that would otherwise need identical URLs. For example: POST /{version}/{client}/assets/deleteasset/{id} - this is very bad practice, instead use the below DELETE /{version}/{client}/assets/{id} - this performs a soft delete on the asset with the specified id DELETE /{version}/{client}/assets/{id}/_force - this performs a hard delete on the same asset The last case is effectively a variation on the previous action, so adding an extra path component for differentiation is acceptable. Use the convention of adding an underscore to signal this variation. Filtering and pagination Wherever possible, a GET request should always be used for reading data. If an endpoint represents a collection of objects, (e.g. /{version}/{client}/assets might represent all Assets for a client), then use query string parameters to allow the caller to retrieve the objects they need. Use ?skip and ?take parameters to allow the caller to perform pagination Use ?sortBy and ?sortDir parameters to allow the caller to specify sort ordering Provide whatever parameters make sense in the context of the use-case to allow filtering. E.g. you might provide ?collections=A,B to filter by collection A or collection B ?status=InComplete,ForReview to filter by assets where the status is Incomplete for For Review A POST request should only ever be used in the case where the maximum length of a query string is insufficient to supply the complexity of the parameters. Make API Endpoints as generic as possible Try to avoid having multiple endpoints that do similar things. E.g. instead of having multiple endpoints to get a different set of assets for different purposes, instead extend the existing endpoint to provide more flexibility. E.g. assume we have the following existing endpoint that is already used for Manage Assets: GET /{version}/{client}/assets - Assumes filtering and pagination are provided by Query String parameters Imagine then that someone needed to get a list of assets for promotion to a website; the example above might not quite meet their needs as it includes metadata-only records for example, so they decide to create a new endpoint specific to their use-case. GET /{version}/{client}/assets/for-promotion/{WebsiteKey} However this isn't ideal, as it creates more code that needs to be maintained and tested. Instead they could simply extend the existing endpoint to add a ?includeMetadataOnly=false query string value to make sure the existing endpoint works for them too.","title":"Endpoint Naming Conventions"},{"location":"6.-Engineering/Quality-Standards/Endpoint-Naming-Conventions/#endpoint-path-conventions","text":"All our microservice endpoints should start with a version (e.g. v1 , v2 etc.), which will have different uses depending on the context. Most microservice endpoints will also relate directly either to a specific Client or specific Website ; although some microservices don't require this (e.g. the File Validation service): /{version}/{ClientKey}/rest/of/url - Version and ClientKey /{version}/{WebsiteKey}/rest/of/url - Version and WebsiteKey /{version}/rest/of/url - Version only Requests paths should use plural nouns, and indicate the hierarchy of the objects that they represent as much as possible. It can be helpful to think of every endpoint path as representing a specific object (or a specific set of objects). /{version}/{client}/jobs - Get all Jobs /{version}/{client}/jobs/{JobId} - Get details for a single Job /{version}/{client}/jobs/{JobId}/items - Get all the Items for a single Job /{version}/{client}/jobs/{JobId}/items/{ItemId} - Get details for a single Item within a given Job NOTE : avoid using verbs in endpoint paths whereever possible. E.g. don't use getasset/{assetId} because the get is already in the HTTP verb. This leads nicely onto the following section...","title":"Endpoint path conventions"},{"location":"6.-Engineering/Quality-Standards/Endpoint-Naming-Conventions/#use-the-relevant-http-verb-for-each-action","text":"Use GET requests for reading data POST requests should be creating new items or uploading files PUT requests should be used for updating existing items DELETE requests should be used for deleting data If we consider each endpoint's path to be a specific object, then it makes sense that each HTTP verb is performing the relevant action on that object. E.g. a GET request will retrieve the object, a PUT request will modify the same object, and a DELETE request would delete that same object. For GET requests, only identifiers (e.g. numerical IDs or Paths) should go in to the URL; filters, sorting or pagination (i.e. skip and take ) parameters should be included as Query String values (implement the IQueryStringRequest interface). In extreme cases it might be necessary to use a POST request to include all the required parameters but avoid doing this if at all possible as it strictly speaking the wrong verb. Some examples below: /{version}/{client}/assets/{skip}/{take} - instead use /{version}/{client}/assets?skip={skip}&take={take} In principle, there could be several identical looking URLs - but using different HTTP Verbs would perform different actions. E.g. GET /{version}/{client}/jobs - Get all jobs. This may include pagination or filtering as query string GET /{version}/{client}/jobs/{JobId} - Get details for a specific job PUT /{version}/{client}/jobs/{JobId} - Update a job (e.g. set its status to Complete - the exact properties to be updated would live in the request body) DELETE /{version}/{client}/jobs/{JobId} - Delete the job Wherever possible avoid putting an action into the URL if it can be indicated by the HTTP Verb. The only exception to this is where there are multiple similar actions that would otherwise need identical URLs. For example: POST /{version}/{client}/assets/deleteasset/{id} - this is very bad practice, instead use the below DELETE /{version}/{client}/assets/{id} - this performs a soft delete on the asset with the specified id DELETE /{version}/{client}/assets/{id}/_force - this performs a hard delete on the same asset The last case is effectively a variation on the previous action, so adding an extra path component for differentiation is acceptable. Use the convention of adding an underscore to signal this variation.","title":"Use the relevant HTTP Verb for each action."},{"location":"6.-Engineering/Quality-Standards/Endpoint-Naming-Conventions/#filtering-and-pagination","text":"Wherever possible, a GET request should always be used for reading data. If an endpoint represents a collection of objects, (e.g. /{version}/{client}/assets might represent all Assets for a client), then use query string parameters to allow the caller to retrieve the objects they need. Use ?skip and ?take parameters to allow the caller to perform pagination Use ?sortBy and ?sortDir parameters to allow the caller to specify sort ordering Provide whatever parameters make sense in the context of the use-case to allow filtering. E.g. you might provide ?collections=A,B to filter by collection A or collection B ?status=InComplete,ForReview to filter by assets where the status is Incomplete for For Review A POST request should only ever be used in the case where the maximum length of a query string is insufficient to supply the complexity of the parameters.","title":"Filtering and pagination"},{"location":"6.-Engineering/Quality-Standards/Endpoint-Naming-Conventions/#make-api-endpoints-as-generic-as-possible","text":"Try to avoid having multiple endpoints that do similar things. E.g. instead of having multiple endpoints to get a different set of assets for different purposes, instead extend the existing endpoint to provide more flexibility. E.g. assume we have the following existing endpoint that is already used for Manage Assets: GET /{version}/{client}/assets - Assumes filtering and pagination are provided by Query String parameters Imagine then that someone needed to get a list of assets for promotion to a website; the example above might not quite meet their needs as it includes metadata-only records for example, so they decide to create a new endpoint specific to their use-case. GET /{version}/{client}/assets/for-promotion/{WebsiteKey} However this isn't ideal, as it creates more code that needs to be maintained and tested. Instead they could simply extend the existing endpoint to add a ?includeMetadataOnly=false query string value to make sure the existing endpoint works for them too.","title":"Make API Endpoints as generic as possible"},{"location":"6.-Engineering/Quality-Standards/Exceptions-Best-Practices/","text":"Fundementals Read and be familiar with the following: Exception Fundamentals Using Exceptions Fundamentals Exception Handling Fundamentals Creating and Throwing Exceptions Fundamentals Compiler-generated Exceptions Fundamentals Best Practices for Exceptions Using Standard Exception Types Best Practises Engineers should follow Microsoft Exception Best Practises. Engineers should not throw System.Exception, System.SystemException, System.NullReferenceException, or System.IndexOutOfRangeException in their own code. Engineers should throw predefined Exceptions when it is suitable to do so e.g. ArgumentNullException Engineers should only create Exceptions when predefined ones do not exist, but should priorities specificity over a less detailed generic predefined exception. e.g. Create custom OutOfMoneyException instead of InvalidOperationException(\"OutOfMoney\") Engineers should only catch specific types of exceptions not a general Exception catch. Engineers should use finally blocks to clean up resources even if no exceptions have been caught.","title":"Fundementals"},{"location":"6.-Engineering/Quality-Standards/Exceptions-Best-Practices/#fundementals","text":"Read and be familiar with the following: Exception Fundamentals Using Exceptions Fundamentals Exception Handling Fundamentals Creating and Throwing Exceptions Fundamentals Compiler-generated Exceptions Fundamentals Best Practices for Exceptions Using Standard Exception Types","title":"Fundementals"},{"location":"6.-Engineering/Quality-Standards/Exceptions-Best-Practices/#best-practises","text":"Engineers should follow Microsoft Exception Best Practises. Engineers should not throw System.Exception, System.SystemException, System.NullReferenceException, or System.IndexOutOfRangeException in their own code. Engineers should throw predefined Exceptions when it is suitable to do so e.g. ArgumentNullException Engineers should only create Exceptions when predefined ones do not exist, but should priorities specificity over a less detailed generic predefined exception. e.g. Create custom OutOfMoneyException instead of InvalidOperationException(\"OutOfMoney\") Engineers should only catch specific types of exceptions not a general Exception catch. Engineers should use finally blocks to clean up resources even if no exceptions have been caught.","title":"Best Practises"},{"location":"6.-Engineering/Quality-Standards/Logging-Best-Practise/","text":"Dotnet Best Practise Engineers should use Log message template when generating log messages (See Here) TypeScript Best Practice Engineers should not use console.log directly, as this can be visible by end users. Alternatively we should be using the Tracer objects from the Eden.Scripts library. (See Monitoring Tips) General Best Practice Engineers should provided enough information engineers understand what was happening when reading the log e.g. Log the client key responsible for the call _logger.LogInformation(\"Create Controlled Vocabulary '{VocabName}' for client {clientKey}\", request.ControlledVocabulary.Name, clientKey); Engineers should use the correct log level when generating logs (see here)","title":"Logging Best Practise"},{"location":"6.-Engineering/Quality-Standards/Logging-Best-Practise/#dotnet-best-practise","text":"Engineers should use Log message template when generating log messages (See Here)","title":"Dotnet Best Practise"},{"location":"6.-Engineering/Quality-Standards/Logging-Best-Practise/#typescript-best-practice","text":"Engineers should not use console.log directly, as this can be visible by end users. Alternatively we should be using the Tracer objects from the Eden.Scripts library. (See Monitoring Tips)","title":"TypeScript Best Practice"},{"location":"6.-Engineering/Quality-Standards/Logging-Best-Practise/#general-best-practice","text":"Engineers should provided enough information engineers understand what was happening when reading the log e.g. Log the client key responsible for the call _logger.LogInformation(\"Create Controlled Vocabulary '{VocabName}' for client {clientKey}\", request.ControlledVocabulary.Name, clientKey); Engineers should use the correct log level when generating logs (see here)","title":"General Best Practice"},{"location":"6.-Engineering/Quality-Standards/Unit-%26-Integration-Testing/","text":"[[ TOC ]] Unit Tests Unit testing is a fundamental tool in every egnineer's toolbox. Unit tests not only help us test our code, they encourage good design practices, reduce the chances of bugs reaching production, and can even serve as examples or documentation on how code functions. Properly written unit tests can also improve egnineer efficiency. It is no secret that writing unit tests is hard, and even harder to write well. Writing unit tests also increases the development time for every feature. So why should we write them? Unit tests should: reduce costs by catching bugs earlier and preventing regressions increase egnineer confidence in changes speed up the egnineer inner loop act as documentation as code Unit tests should also be very predictable (i.e. any failures should indicate broken code) and very fast (unit testing a ) Integration tests Integration testing is a software testing methodology used to determine how well individually developed components, or modules of a system communicate with each other. This method of testing confirms that an aggregate of a system, or sub-system, works together correctly or otherwise exposes erroneous behavior between two or more units of code. Code Coverage The easiest way of quantatively measuring the quality of automated tests is by examining code covege. Whilst a high code coverage percentage does not necessarily mean that the tests are of a high quality, it is certainly true that a low code coverage is indicative of there not being enough automated tests! Note that we are more interested in branch coverage than line coverage. There are two ways that our repositories measure and report on code coverage: By generating an HTML report By checking that committed code meets a minimum coverage in GitHub Actions Note : if the repository does not contain the tools to measure code coverage, follow this guide for .NET repositories to add it. The HTML report can be very simply generated by selecting the Run Code Coverage item from the Tools menu in Visual Studio. This will take a few moments runs the tests and generates the report and displays it in the Visual Studio window. The report gives a detailed breakdown of test coverage by class. Clicking through to a class will then break this down by method and individual line. The report can then be used to: Give an indication of parts of the code that are not tested (and thus need new tests or test cases writing) A methods having a high number of branches (or high cyclometric complexity) is potentially and indication that the code in the method should be broken down or split out into other classes Provide the current code coverage The .github/workflows/tests.yml workflow definition file contains the step to validate code coverage, and sets the minimum coverage level for the repository. Code Coverage Requirements As part of our Quality Standards, we require that around 80% of our code is covered by automated testing. When working on a repository, an engineer should ensure that any new code is also covered by new automated tests to maintain or increase this coverage. Existing repositories may not hit this 80% coverage threshold, so expecting to meet it when making a change to an existing repository may not be realistic. When committing code, an engineer should consult the latest coverage report to get the latest branch coverage, and update the BRANCH_THRESHOLD variable in the tests.yml .","title":"Unit & Integration Testing"},{"location":"6.-Engineering/Quality-Standards/Unit-%26-Integration-Testing/#unit-tests","text":"Unit testing is a fundamental tool in every egnineer's toolbox. Unit tests not only help us test our code, they encourage good design practices, reduce the chances of bugs reaching production, and can even serve as examples or documentation on how code functions. Properly written unit tests can also improve egnineer efficiency. It is no secret that writing unit tests is hard, and even harder to write well. Writing unit tests also increases the development time for every feature. So why should we write them? Unit tests should: reduce costs by catching bugs earlier and preventing regressions increase egnineer confidence in changes speed up the egnineer inner loop act as documentation as code Unit tests should also be very predictable (i.e. any failures should indicate broken code) and very fast (unit testing a )","title":"Unit Tests"},{"location":"6.-Engineering/Quality-Standards/Unit-%26-Integration-Testing/#integration-tests","text":"Integration testing is a software testing methodology used to determine how well individually developed components, or modules of a system communicate with each other. This method of testing confirms that an aggregate of a system, or sub-system, works together correctly or otherwise exposes erroneous behavior between two or more units of code.","title":"Integration tests"},{"location":"6.-Engineering/Quality-Standards/Unit-%26-Integration-Testing/#code-coverage","text":"The easiest way of quantatively measuring the quality of automated tests is by examining code covege. Whilst a high code coverage percentage does not necessarily mean that the tests are of a high quality, it is certainly true that a low code coverage is indicative of there not being enough automated tests! Note that we are more interested in branch coverage than line coverage. There are two ways that our repositories measure and report on code coverage: By generating an HTML report By checking that committed code meets a minimum coverage in GitHub Actions Note : if the repository does not contain the tools to measure code coverage, follow this guide for .NET repositories to add it. The HTML report can be very simply generated by selecting the Run Code Coverage item from the Tools menu in Visual Studio. This will take a few moments runs the tests and generates the report and displays it in the Visual Studio window. The report gives a detailed breakdown of test coverage by class. Clicking through to a class will then break this down by method and individual line. The report can then be used to: Give an indication of parts of the code that are not tested (and thus need new tests or test cases writing) A methods having a high number of branches (or high cyclometric complexity) is potentially and indication that the code in the method should be broken down or split out into other classes Provide the current code coverage The .github/workflows/tests.yml workflow definition file contains the step to validate code coverage, and sets the minimum coverage level for the repository.","title":"Code Coverage"},{"location":"6.-Engineering/Quality-Standards/Unit-%26-Integration-Testing/#code-coverage-requirements","text":"As part of our Quality Standards, we require that around 80% of our code is covered by automated testing. When working on a repository, an engineer should ensure that any new code is also covered by new automated tests to maintain or increase this coverage. Existing repositories may not hit this 80% coverage threshold, so expecting to meet it when making a change to an existing repository may not be realistic. When committing code, an engineer should consult the latest coverage report to get the latest branch coverage, and update the BRANCH_THRESHOLD variable in the tests.yml .","title":"Code Coverage Requirements"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/","text":"[[ TOC ]] Purpose & Audience Purpose: Outlines the current source control techniques for our shared codebases, referred to as our \"branching strategy\". Audience: This is targeted at any member of the team who performs work against any shared codebases, including code changes and reviews. Versioning Our shared code packages use Semantic Versioning (or SemVer ) compatible version numbers. These are of the format MAJOR.MINOR.PATCH , and one should increment different parts the version depeneding on the type of changes being made: MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. Our CI workflows will only trigger when you merge a pull request into the main branch, and will not overwrite an existing package with an identical version. In practice this means that you must ensure the version number of a package is update every time you make a pull request. As part of the pull request, you should indicate how the version number is changing, and why. When to update version numbers Supporting new feature development When working on a new feature, the most likely types of updates that need to be made to shared code will be one of the following: Creating new Request and Response classes for microservice communication Updating shared schema definitions (e.g. adding a column to a table) Adding new capabilities to other pieces of shared code In this case you should increment the MINOR version and reset the PATCH to 0. If incremental changes need to be made as part of the same feature .e.g as part of peer review comments, or to address defects raised as part of signing), simply increment the PATCH number for each incremental change that needs to be made. Implementing Fixes When working on a fix of some description (e.g. a fix to an extension method) that should be backwards compatible, simply increment the PATCH number. Breaking Changes or Major Updates Examples of a MAJOR update would include: Wholesale refactoring of the package (e.g. splitting some functionality out into a new package) Framework version upgrades (e.g. updating the package to be comaptible with .NET 5) IMPORTANT: in addition to the examples noted above, should any change that would usually be a MINOR or PATCH change break compilation on any solution depending on the package, then you should instead increment MAJOR version. A breaking change can be thought of as any change the requires code that depends upon it to be updated. Updating Packages Once a pull request has been approved and merged into main , any updated packages will be automatically built and published to GitHub Packages, ready to be pulled in by any other repositories that need them. Updating Repositories Once a new version of a package has been created, you need to ensure that the relevant other repositories are updated to use it. Applying MAJOR or MINOR version updates Usually, any update to packages that requires a MAJOR or MINOR version change will be made with one or more repositories in mind. E.g. if creating new Request & Response classes to implement a new endpoint, at least the microservice implementing the new endpoint and one or more other microservice will need to access the new classes. For each repository that needs the new code, update the relevant csproj files within the codebase as follows: <ItemGroup> <PackageReference Include=\"Quartex.Package.A\" Version=\"MAJOR.MINOR.*\" /> </ItemGroup> This will ensure the service is built against the MAJOR.MINOR version, whilst automatically ensuring it is pulling in any fixes and updates. Applying PATCH version updates Since we are using wildcard versioning, there is no need to explicitly update repositories to pull in the latest PATCH number. Branching Source Control Code is managed and stored using Git and GitHub. Strategy Basics Our versioning strategy means that every change produces a unique package version, and depedant repositories can specifically target known versions of each package. This means our branching strategy for shared code can be much simpler than the one used for other repository types. Branch Key Purpose Naming Main Reflects the state of the code that produced the last published package. main Work Contains only the work of an individual within the team. Work is only ever done against a work branch work/{Engineer Name}/{Change Name} Each engineer who needs to make changes to a package must create their own Work branch off Main , against which they can commit all changes. Regular and small commits are recommended, and should be regularly pushed . Each commit must be accompanied by a useful description of the change made. When changes to the package have been completed, and a new version of the package built, the engineer should submit a Pull Request back to Main . Follow the pull request instructions Fill in the details requested as part of the template pull request Another member of the team should then perform a peer review on the work Any changes requested as part of the peer review should be committed to the Work branch. When the review is completed (including any agreed changes, as above), the pull request can be merged into Main , and new package versions will be automatically created","title":"Branching & Versioning Shared Code Repositories"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#purpose-audience","text":"Purpose: Outlines the current source control techniques for our shared codebases, referred to as our \"branching strategy\". Audience: This is targeted at any member of the team who performs work against any shared codebases, including code changes and reviews.","title":"Purpose &amp; Audience"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#versioning","text":"Our shared code packages use Semantic Versioning (or SemVer ) compatible version numbers. These are of the format MAJOR.MINOR.PATCH , and one should increment different parts the version depeneding on the type of changes being made: MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. Our CI workflows will only trigger when you merge a pull request into the main branch, and will not overwrite an existing package with an identical version. In practice this means that you must ensure the version number of a package is update every time you make a pull request. As part of the pull request, you should indicate how the version number is changing, and why.","title":"Versioning"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#when-to-update-version-numbers","text":"","title":"When to update version numbers"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#supporting-new-feature-development","text":"When working on a new feature, the most likely types of updates that need to be made to shared code will be one of the following: Creating new Request and Response classes for microservice communication Updating shared schema definitions (e.g. adding a column to a table) Adding new capabilities to other pieces of shared code In this case you should increment the MINOR version and reset the PATCH to 0. If incremental changes need to be made as part of the same feature .e.g as part of peer review comments, or to address defects raised as part of signing), simply increment the PATCH number for each incremental change that needs to be made.","title":"Supporting new feature development"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#implementing-fixes","text":"When working on a fix of some description (e.g. a fix to an extension method) that should be backwards compatible, simply increment the PATCH number.","title":"Implementing Fixes"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#breaking-changes-or-major-updates","text":"Examples of a MAJOR update would include: Wholesale refactoring of the package (e.g. splitting some functionality out into a new package) Framework version upgrades (e.g. updating the package to be comaptible with .NET 5) IMPORTANT: in addition to the examples noted above, should any change that would usually be a MINOR or PATCH change break compilation on any solution depending on the package, then you should instead increment MAJOR version. A breaking change can be thought of as any change the requires code that depends upon it to be updated.","title":"Breaking Changes or Major Updates"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#updating-packages","text":"Once a pull request has been approved and merged into main , any updated packages will be automatically built and published to GitHub Packages, ready to be pulled in by any other repositories that need them.","title":"Updating Packages"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#updating-repositories","text":"Once a new version of a package has been created, you need to ensure that the relevant other repositories are updated to use it.","title":"Updating Repositories"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#applying-major-or-minor-version-updates","text":"Usually, any update to packages that requires a MAJOR or MINOR version change will be made with one or more repositories in mind. E.g. if creating new Request & Response classes to implement a new endpoint, at least the microservice implementing the new endpoint and one or more other microservice will need to access the new classes. For each repository that needs the new code, update the relevant csproj files within the codebase as follows: <ItemGroup> <PackageReference Include=\"Quartex.Package.A\" Version=\"MAJOR.MINOR.*\" /> </ItemGroup> This will ensure the service is built against the MAJOR.MINOR version, whilst automatically ensuring it is pulling in any fixes and updates.","title":"Applying MAJOR or MINOR version updates"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#applying-patch-version-updates","text":"Since we are using wildcard versioning, there is no need to explicitly update repositories to pull in the latest PATCH number.","title":"Applying PATCH version updates"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#branching","text":"","title":"Branching"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#source-control","text":"Code is managed and stored using Git and GitHub.","title":"Source Control"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#strategy-basics","text":"Our versioning strategy means that every change produces a unique package version, and depedant repositories can specifically target known versions of each package. This means our branching strategy for shared code can be much simpler than the one used for other repository types. Branch Key Purpose Naming Main Reflects the state of the code that produced the last published package. main Work Contains only the work of an individual within the team. Work is only ever done against a work branch work/{Engineer Name}/{Change Name} Each engineer who needs to make changes to a package must create their own Work branch off Main , against which they can commit all changes. Regular and small commits are recommended, and should be regularly pushed . Each commit must be accompanied by a useful description of the change made. When changes to the package have been completed, and a new version of the package built, the engineer should submit a Pull Request back to Main . Follow the pull request instructions Fill in the details requested as part of the template pull request Another member of the team should then perform a peer review on the work Any changes requested as part of the peer review should be committed to the Work branch. When the review is completed (including any agreed changes, as above), the pull request can be merged into Main , and new package versions will be automatically created","title":"Strategy Basics"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Versioning-Microservices/","text":"This document has yet to be written. Microservice versioning is scheduled for review in Q4 2021","title":"This document has yet to be written."},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Versioning-Microservices/#this-document-has-yet-to-be-written","text":"Microservice versioning is scheduled for review in Q4 2021","title":"This document has yet to be written."},{"location":"6.-Engineering/Structure-and-Patterns/Caching/","text":"[[ TOC ]] Basic Cache Usage Caching is primarily performed using the ICache interface. To used cached values in code, first use Dependency Injection to retrieve an ICache instance. In many ways, the cache can be thought of just like a Dictionary<string, anything> that stores values that take a long time to calculate or retrieve from remote locations. In this analogy, we use a string cache key to store and retrieve these results. Most of the time the best apracoh is to use the T Get<T>(string cacheKey, Func<T> getter) method. This call takes two parameters: a string (cache key) which behaves just like the dictionary key, and a getter: a function which calculates the value if it does not exist already in the cache. string cacheKey = \"global.my.cache.key\"; // cache key that probably contains ClientKey or WebsiteKey var cachedValue = _cache.Get(cacheKey, () => { // expensive, long running piece of code to calculte value return calculateValue; }); This call will first check to see if the cache contains a value with the specified key. If the value can be found, it is returned immediately If it cannot be found within the cache, it will run the getter to calculte the required value and Store the value in the cache for future quick retrieval Returns the value to the calling code The first time a given cached value is requested, it will not yet exist in the cache, and go through route 2. above. On subsequant requests for the same cached value, it now exists and will go through route 1. Cache Scope Quartex has several levels of caching, called \"Cache Scopes\". Each Scope typically requires more time/resource to store/retrieve than the previous Scope; as such it makes sense to store larger/more expensive to compute values in a higher Scope: Disabled : the value is never cached, and the getter() will always be called Ephemeral : cached values are stored for the duration of the current HTTP Request, within HttpContext.Items . If the same value is requested in another HTTP Request, it will need to be re-calculated InMemory : values are stored for a configurable length of time in Memory, but only within the same application. If the same value is requested within another application, or in another instance of the same application (e.g. on a different server), it will need to be re-calculated. Distributed : values are stored for a configurable length of time in Redis. NOTE: If you do not configure a new Cache Key (or it is not convered by configuration for an existing prefix), the default behaviour is to use NotCached for the cache scope. If it seems like a newly cached value is never appearing in Redis, or your getter() is getting called every time; ensure the cache key is covered by a configuration entry. See below for configuration. Configuration of Cached Values Cache Scope is configured along with expiry times in quartex.cache.json . Related sets of cache keys are configured by common prefixes. E.g. in the example below, any value stored with a cache key that begins sw.CSS.SiteStyles will be cached in Redis, with an expiry time of 3 hours: \"sw.CSS.SiteStyles\": { \"Scope\": \"Distributed\", \"Hours\": 3 } Cache configuration and cache key naming strategies go hand in hand. Typically, we use a convention of the form {Component}.{Category}.{Value}.{ClientKey}.{WebsiteKey} . In the above example, the component is the Static Webservice (abbreviated as sw ), the category is CSS , and the specific value is SiteStyles , finally the Client Key and Website Key are append giving the following: string cacheKey = $\"sw.CSS.SiteStyles.{_website.ClientKey}.{_website.WebsiteKey}\"; Cache Key Naming When creating a cache key, always be careful to ensure that the ky is unique, as otherwise a value cached for one client/site would then get used for all clients or sites. Imagine caching Site Styles for one front-end site, and having them retrieved for a completely unrelated front-end. A cache key should always include the Client Key and Website Key as appropriate, and should make reference to individual identifiers where appropriate too. The following examples are for the HTML of a static page, and the document details and fields retrieved from Elastic Search. The static page uses it's Path as the unique individual identifier, and the document response uses the document's ID. string htmlCacheKey = $\"fe.Html.{_website.ClientKey}{_website.WebsiteKey}.Static.{_page.Path}\"; string documentResponse = $\"dms.InitDataRequest.{websiteKey}.{documentId}\"; Cache Invalidation When caching data, we need to make sure that outdated or stale data does not stay in the cache. There are some exceptions to this rule; for example with the DAM Dashboard we have explicitly decided that having a the UI return potentially out-of-date results quickly is preferable to always serving up fresh data very slowly. However, in most cases if you are caching data you will need to consider when and how to clear the cached values when they update. In Quartex the easiest method for invalidating cache data is to use the IEventBus to raise a custom event, along with some configuration in quartex.cache.json to clear certain cache keys whenever that event is raised. Clearing cache entries via the IEventBus Raising an event works as follows: use the IEventBus to call the Raise method, and pass in an Event object specifying a Name and a dictionary of Arguments: _eventBus.Raise(new Event { Name = EventNames.FrontEndStaticPagePublish, Arguments = new Dictionary<string, string> { { \"ClientKey\", client.ClientKey }, { \"WebsiteKey\", websiteKey }, { \"PageId\", page.Id.ToString() }, { \"PagePath\", page.Path } } }); In this example, we are raising an event to broadcast the fact that a Static Page has just been published. Note that we pass in a dictionary of relevant arguments which we can reference later when defining which cache entries to cache. In the above example, EventNames.FrontEndStaticPagePublish is a static string that returns \"evt.fe.staticpagepublished\" . To configure cache invalidation, we use the Event Name as the key, and define an array of cache keys that need to be cleared. The keys to be cleared can then reference the event's arguments by putting the argument's name in braces, e.g. {ArgumentName} . It is also possible to use a * as a wildcard. { \"CacheSettings\": { \"CacheInvalidation\": { // other items... \"evt.fe.staticpagepublished\": [ \"common.WebsiteTenant.{WebsiteKey}\", \"fe.Html.{ClientKey}.{WebsiteKey}.*\" ] // other items... } } } In this worked example, when a static page is published, we want to clear the followinag things: The WebsiteInfo object for the current website All cache HTML for the website - note that this is using a wildcard Creating new Events to clear caches When you have worked out the best time to clear a set of related cache keys, follow the process below. Create a new static string in Quartex.Common\\EventBus\\EventNames.cs Raise the event in your code using the IEventBus and using the EventNames static string you created above Add a configuration entry to Quartex.Configuration\\quartex.cache.json The quartex.cache.json entry needs to use the raw event name (i.e. format like evt.component.eventname ) instead of the EventNames.SomeEventName notation (which is only applicable in C#). Clearing Local vs Distributed caches There are a range of different cache scopes and clearing each of these is done differently. Disabled - these items are not cached, so there is nothing to clear Ephemeral - these items are only stored for the duration of an HTTP request, so don't hang around long enough to need clearing InMemory - the Event based system means that any other Quartex component that has an In Memory key cached will receive the event and clear its own cached copy Distributed - these items are stored in Redis, so whichever Quartex component is raising the event will also remove the value from Redis All of this is handled by the Shared Code Library for you, so raising the relevant Event and adding the relevant configuration is all that needs to be done from a developer perspective. Caching Service Requests NOTE : the methods described below will only work in whilst using the IServiceClient in C#. The methods are not applicable to Type Script microservice calls. As more of Quartex is being built using a microservice architecture, we are finding that microservice requests can be a very useful point to add caching to the system. Most microservice requests will fall into one of two categories: We have GET type requests to retrieve information, or Read operations We POST/PUT/DELETE etc type requests that update information, or Write operations Read operations are the kind of things we might want to cache, and Write operations by definition will update data that may be cached (making the cached information out-of-date) and thus will be operations where we want to invaliate cached data. Using the Service Client to cache data Caching microservice requests and having them invalidate data is achieved by simply making the IServiceRequest classes implement some additional interfaces; once these setup steps are complete, everything is handled by the IServiceClient and nothing additional needs to be done from a developer perspective. To illustrate how these can be used together, we will use the example of the Features endpoints on the Clients Microservice. We will show how to cache the results of the GetFeaturesRequest (a Read operation), and show how to clear the cache whenever we use SetFeaturesRequest (a Write operation) to modify data. Caching the results of a Microservice Call Firstly, implement the ICacheableRequest on your request class. This requires us to implement a single method, GetCacheKey() which simply enough generates a string that will later be used as the cache key. Bear in mind that good cache key naming is essential. In this case, we have a ClientKey property to use to ensure we cache the features for different clients separately. public class GetFeaturesRequest : IServiceRequest<GetFeaturesResponse>, IInternalRequest, ICacheableRequest { // normal IServiceRequest Properties ... public string ClientKey { get; set; } public string GetCacheKey() { return $\"client.Features.{ClientKey}\"; } } Having created a new cache key, we also need to configure the cache location and expiry in quartex.cache.json : { \"CacheSettings\": { \"KeySettings\":{ // Any entries in KeySettings apply to any cached values that start with the following keys. // So \"client.Features\" will apply to \"client.Features.{anything...}\" \"client.Features\": { \"Scope\": \"Distributed\", \"Hours\": 2, \"Priority\": 1, \"RetryLimit\": 1 } } } } Once this is setup, any time any component uses the IServiceClient to make make a GetFeaturesRequest , the IServiceClient will first check to see if the result is in the cache; if yes, it doesn't bother making the actual microservice request and simply returns the cached result. If the results are not in the cache, the IServiceClient will make the microservice request as normal, and if it gets a Successful response, the result will be cached for future occasions. {{% alert theme=\"info\" %}} NOTE : only successful results are cached. So if the microservice responds with a 404 Not Found response or a 500 Internal Server Error for example, the result will not be cached. {{%/alert%}} Clearing the cache when making a Microservice Call Cache clearing is done via the mechanism of raising an event, so we will need to create a new Event Name in the Quartex.Common/EventBus/EventNames.cs file: public static class EventNames { // other existing event names.. public static string ClientFeaturesEdit => \"evt.clients.featureschanged\"; } Similarly, in order to make our Write operation clear invalid cache keys, we need to make the request class extend IEventRaisingRequest and implement the Event property. public class SetFeatureRequest : IServiceRequest<SetFeatureResponse>, IInternalRequest, IEventRaisingRequest { // normal IServiceRequest Properties ... public string ClientKey { get; set; } public Event Event => new Event() { Name = EventNames.ClientFeaturesEdit, Arguments = new Dictionary<string, string> { { \"ClientKey\", ClientKey } } }; } Similarly, we will need to configure the invalidation section of quartex.cache.json . As before, we need to use the raw event name (instead of referencing it via the EventNames static variable). { \"CacheSettings\": { \"CacheInvalidation\": { \"evt.clients.featureschanged\": [ \"client.Features.{ClientKey}\", \"client.Features.{ClientKey}.*\" ] } } } Now, whenever any component uses the IServiceClient to make a SetFeatureRequest , if the response from the microservice comes back as successful, our event will be raised and any related caches cleared.","title":"Caching"},{"location":"6.-Engineering/Structure-and-Patterns/Caching/#basic-cache-usage","text":"Caching is primarily performed using the ICache interface. To used cached values in code, first use Dependency Injection to retrieve an ICache instance. In many ways, the cache can be thought of just like a Dictionary<string, anything> that stores values that take a long time to calculate or retrieve from remote locations. In this analogy, we use a string cache key to store and retrieve these results. Most of the time the best apracoh is to use the T Get<T>(string cacheKey, Func<T> getter) method. This call takes two parameters: a string (cache key) which behaves just like the dictionary key, and a getter: a function which calculates the value if it does not exist already in the cache. string cacheKey = \"global.my.cache.key\"; // cache key that probably contains ClientKey or WebsiteKey var cachedValue = _cache.Get(cacheKey, () => { // expensive, long running piece of code to calculte value return calculateValue; }); This call will first check to see if the cache contains a value with the specified key. If the value can be found, it is returned immediately If it cannot be found within the cache, it will run the getter to calculte the required value and Store the value in the cache for future quick retrieval Returns the value to the calling code The first time a given cached value is requested, it will not yet exist in the cache, and go through route 2. above. On subsequant requests for the same cached value, it now exists and will go through route 1.","title":"Basic Cache Usage"},{"location":"6.-Engineering/Structure-and-Patterns/Caching/#cache-scope","text":"Quartex has several levels of caching, called \"Cache Scopes\". Each Scope typically requires more time/resource to store/retrieve than the previous Scope; as such it makes sense to store larger/more expensive to compute values in a higher Scope: Disabled : the value is never cached, and the getter() will always be called Ephemeral : cached values are stored for the duration of the current HTTP Request, within HttpContext.Items . If the same value is requested in another HTTP Request, it will need to be re-calculated InMemory : values are stored for a configurable length of time in Memory, but only within the same application. If the same value is requested within another application, or in another instance of the same application (e.g. on a different server), it will need to be re-calculated. Distributed : values are stored for a configurable length of time in Redis. NOTE: If you do not configure a new Cache Key (or it is not convered by configuration for an existing prefix), the default behaviour is to use NotCached for the cache scope. If it seems like a newly cached value is never appearing in Redis, or your getter() is getting called every time; ensure the cache key is covered by a configuration entry. See below for configuration.","title":"Cache Scope"},{"location":"6.-Engineering/Structure-and-Patterns/Caching/#configuration-of-cached-values","text":"Cache Scope is configured along with expiry times in quartex.cache.json . Related sets of cache keys are configured by common prefixes. E.g. in the example below, any value stored with a cache key that begins sw.CSS.SiteStyles will be cached in Redis, with an expiry time of 3 hours: \"sw.CSS.SiteStyles\": { \"Scope\": \"Distributed\", \"Hours\": 3 } Cache configuration and cache key naming strategies go hand in hand. Typically, we use a convention of the form {Component}.{Category}.{Value}.{ClientKey}.{WebsiteKey} . In the above example, the component is the Static Webservice (abbreviated as sw ), the category is CSS , and the specific value is SiteStyles , finally the Client Key and Website Key are append giving the following: string cacheKey = $\"sw.CSS.SiteStyles.{_website.ClientKey}.{_website.WebsiteKey}\";","title":"Configuration of Cached Values"},{"location":"6.-Engineering/Structure-and-Patterns/Caching/#cache-key-naming","text":"When creating a cache key, always be careful to ensure that the ky is unique, as otherwise a value cached for one client/site would then get used for all clients or sites. Imagine caching Site Styles for one front-end site, and having them retrieved for a completely unrelated front-end. A cache key should always include the Client Key and Website Key as appropriate, and should make reference to individual identifiers where appropriate too. The following examples are for the HTML of a static page, and the document details and fields retrieved from Elastic Search. The static page uses it's Path as the unique individual identifier, and the document response uses the document's ID. string htmlCacheKey = $\"fe.Html.{_website.ClientKey}{_website.WebsiteKey}.Static.{_page.Path}\"; string documentResponse = $\"dms.InitDataRequest.{websiteKey}.{documentId}\";","title":"Cache Key Naming"},{"location":"6.-Engineering/Structure-and-Patterns/Caching/#cache-invalidation","text":"When caching data, we need to make sure that outdated or stale data does not stay in the cache. There are some exceptions to this rule; for example with the DAM Dashboard we have explicitly decided that having a the UI return potentially out-of-date results quickly is preferable to always serving up fresh data very slowly. However, in most cases if you are caching data you will need to consider when and how to clear the cached values when they update. In Quartex the easiest method for invalidating cache data is to use the IEventBus to raise a custom event, along with some configuration in quartex.cache.json to clear certain cache keys whenever that event is raised.","title":"Cache Invalidation"},{"location":"6.-Engineering/Structure-and-Patterns/Caching/#clearing-cache-entries-via-the-ieventbus","text":"Raising an event works as follows: use the IEventBus to call the Raise method, and pass in an Event object specifying a Name and a dictionary of Arguments: _eventBus.Raise(new Event { Name = EventNames.FrontEndStaticPagePublish, Arguments = new Dictionary<string, string> { { \"ClientKey\", client.ClientKey }, { \"WebsiteKey\", websiteKey }, { \"PageId\", page.Id.ToString() }, { \"PagePath\", page.Path } } }); In this example, we are raising an event to broadcast the fact that a Static Page has just been published. Note that we pass in a dictionary of relevant arguments which we can reference later when defining which cache entries to cache. In the above example, EventNames.FrontEndStaticPagePublish is a static string that returns \"evt.fe.staticpagepublished\" . To configure cache invalidation, we use the Event Name as the key, and define an array of cache keys that need to be cleared. The keys to be cleared can then reference the event's arguments by putting the argument's name in braces, e.g. {ArgumentName} . It is also possible to use a * as a wildcard. { \"CacheSettings\": { \"CacheInvalidation\": { // other items... \"evt.fe.staticpagepublished\": [ \"common.WebsiteTenant.{WebsiteKey}\", \"fe.Html.{ClientKey}.{WebsiteKey}.*\" ] // other items... } } } In this worked example, when a static page is published, we want to clear the followinag things: The WebsiteInfo object for the current website All cache HTML for the website - note that this is using a wildcard","title":"Clearing cache entries via the IEventBus"},{"location":"6.-Engineering/Structure-and-Patterns/Caching/#creating-new-events-to-clear-caches","text":"When you have worked out the best time to clear a set of related cache keys, follow the process below. Create a new static string in Quartex.Common\\EventBus\\EventNames.cs Raise the event in your code using the IEventBus and using the EventNames static string you created above Add a configuration entry to Quartex.Configuration\\quartex.cache.json The quartex.cache.json entry needs to use the raw event name (i.e. format like evt.component.eventname ) instead of the EventNames.SomeEventName notation (which is only applicable in C#).","title":"Creating new Events to clear caches"},{"location":"6.-Engineering/Structure-and-Patterns/Caching/#clearing-local-vs-distributed-caches","text":"There are a range of different cache scopes and clearing each of these is done differently. Disabled - these items are not cached, so there is nothing to clear Ephemeral - these items are only stored for the duration of an HTTP request, so don't hang around long enough to need clearing InMemory - the Event based system means that any other Quartex component that has an In Memory key cached will receive the event and clear its own cached copy Distributed - these items are stored in Redis, so whichever Quartex component is raising the event will also remove the value from Redis All of this is handled by the Shared Code Library for you, so raising the relevant Event and adding the relevant configuration is all that needs to be done from a developer perspective.","title":"Clearing Local vs Distributed caches"},{"location":"6.-Engineering/Structure-and-Patterns/Caching/#caching-service-requests","text":"NOTE : the methods described below will only work in whilst using the IServiceClient in C#. The methods are not applicable to Type Script microservice calls. As more of Quartex is being built using a microservice architecture, we are finding that microservice requests can be a very useful point to add caching to the system. Most microservice requests will fall into one of two categories: We have GET type requests to retrieve information, or Read operations We POST/PUT/DELETE etc type requests that update information, or Write operations Read operations are the kind of things we might want to cache, and Write operations by definition will update data that may be cached (making the cached information out-of-date) and thus will be operations where we want to invaliate cached data.","title":"Caching Service Requests"},{"location":"6.-Engineering/Structure-and-Patterns/Caching/#using-the-service-client-to-cache-data","text":"Caching microservice requests and having them invalidate data is achieved by simply making the IServiceRequest classes implement some additional interfaces; once these setup steps are complete, everything is handled by the IServiceClient and nothing additional needs to be done from a developer perspective. To illustrate how these can be used together, we will use the example of the Features endpoints on the Clients Microservice. We will show how to cache the results of the GetFeaturesRequest (a Read operation), and show how to clear the cache whenever we use SetFeaturesRequest (a Write operation) to modify data.","title":"Using the Service Client to cache data"},{"location":"6.-Engineering/Structure-and-Patterns/Caching/#caching-the-results-of-a-microservice-call","text":"Firstly, implement the ICacheableRequest on your request class. This requires us to implement a single method, GetCacheKey() which simply enough generates a string that will later be used as the cache key. Bear in mind that good cache key naming is essential. In this case, we have a ClientKey property to use to ensure we cache the features for different clients separately. public class GetFeaturesRequest : IServiceRequest<GetFeaturesResponse>, IInternalRequest, ICacheableRequest { // normal IServiceRequest Properties ... public string ClientKey { get; set; } public string GetCacheKey() { return $\"client.Features.{ClientKey}\"; } } Having created a new cache key, we also need to configure the cache location and expiry in quartex.cache.json : { \"CacheSettings\": { \"KeySettings\":{ // Any entries in KeySettings apply to any cached values that start with the following keys. // So \"client.Features\" will apply to \"client.Features.{anything...}\" \"client.Features\": { \"Scope\": \"Distributed\", \"Hours\": 2, \"Priority\": 1, \"RetryLimit\": 1 } } } } Once this is setup, any time any component uses the IServiceClient to make make a GetFeaturesRequest , the IServiceClient will first check to see if the result is in the cache; if yes, it doesn't bother making the actual microservice request and simply returns the cached result. If the results are not in the cache, the IServiceClient will make the microservice request as normal, and if it gets a Successful response, the result will be cached for future occasions. {{% alert theme=\"info\" %}} NOTE : only successful results are cached. So if the microservice responds with a 404 Not Found response or a 500 Internal Server Error for example, the result will not be cached. {{%/alert%}}","title":"Caching the results of a Microservice Call"},{"location":"6.-Engineering/Structure-and-Patterns/Caching/#clearing-the-cache-when-making-a-microservice-call","text":"Cache clearing is done via the mechanism of raising an event, so we will need to create a new Event Name in the Quartex.Common/EventBus/EventNames.cs file: public static class EventNames { // other existing event names.. public static string ClientFeaturesEdit => \"evt.clients.featureschanged\"; } Similarly, in order to make our Write operation clear invalid cache keys, we need to make the request class extend IEventRaisingRequest and implement the Event property. public class SetFeatureRequest : IServiceRequest<SetFeatureResponse>, IInternalRequest, IEventRaisingRequest { // normal IServiceRequest Properties ... public string ClientKey { get; set; } public Event Event => new Event() { Name = EventNames.ClientFeaturesEdit, Arguments = new Dictionary<string, string> { { \"ClientKey\", ClientKey } } }; } Similarly, we will need to configure the invalidation section of quartex.cache.json . As before, we need to use the raw event name (instead of referencing it via the EventNames static variable). { \"CacheSettings\": { \"CacheInvalidation\": { \"evt.clients.featureschanged\": [ \"client.Features.{ClientKey}\", \"client.Features.{ClientKey}.*\" ] } } } Now, whenever any component uses the IServiceClient to make a SetFeatureRequest , if the response from the microservice comes back as successful, our event will be raised and any related caches cleared.","title":"Clearing the cache when making a Microservice Call"},{"location":"6.-Engineering/Structure-and-Patterns/Distributed-Locks/","text":"[[ TOC ]] Quartex is a distributed system, with multiple microservices doing different things at the same time. This can be a recipe for race conditions to creep in and create strange bugs, especially at scale. NOTE : the IDistributedLock examples below are intended to prevent two identical pieces of code running at the same time. For example an early issue with large scale FTP uploads was that we would sometimes see multiple Asset records for the folder created in the database. The reason for this was multiple threads uploading files within the same folder would all try to create the parent record at the same time. Wrapping a distributed lock around the code creating the parent record solve the issue. As a concrete example of this, imagine two parallel processes importing two files: /folder1/file1.jpg and /folder1/file2.jpg , and assume folder1 doesn't exist yet. Both processes will try to create folder1 the process importing file1.jpg gets there first, and grabs a lock (let's assume the key is called ClientKey.CreateAsset.folder1 ) the process importing file2.jpg gets there a second later, can't get the lock, which means that the folder is already being created (so it doesn't need to do anything at all) Note: it's quite possible that the second process may try to carroy on before folder1 is in the database. In which case it will probably throw an exception, but when it retries a few minutes later it will work fine. The following debug logging shows Task 1 grabbing a lock on a key and the running, then Task 2 attempts to grab the same lock but fails (since Task 1 has the lock on that key); as a result Task 2 does not run at all. Examples of this could be: Stop two instances of the same microservice from running a RegularTask at the same time Prevent two parallel HTTP requests to the same microservice from running the same initialistion In each case, by using the distributed lock methods shown below, the first instance of the code will run, and the second will not (because we know it is already being done). Using the Distributed Lock in C# code You will need to dependency inject an IDistributedLock object into your service, as below: private readonly IDistributedLock _distributedLock; public MyService(IDistributedLock distributedLock) { _distributedLock = distributedLock; } There are three patterns for using the distributed lock. In each case you need to generate a predictable key, and you will also need to specify how long the lock is valid for (which guards against the lock never being released if the calling code dies). string lockKey = $\"asset.Create.{_client.ClientKey}.{asset.Id}\"; // only excecute the following code if you can acquire the lock // the lock is not explicitly released, it will expire after the specified time if (_distributedLock.AcquireLock(lockKey, TimeSpan.FromMinutes(5))) { // code to execute } // attempt to acquire the lock // you MUST explicitly check the HasLock property using (var token = _distributedLock.UsingToken(lockKey, TimeSpan.FromMinutes(5))) { if (token.HasLock) { // code to execute } else { // what to do when we do NOT have the lock? Probably nothing! } } // lock is explicitly released at the end of the using block // similar to the above, but using a lambda // If the lock cannot be acquired, it will simply not do anything await _distributedLock.RunAsync(lockKey, async () => { // code to execute }, 300); Running different code that access the same resource NOTE : use this approach to allow two separate processes to run, but guaranteed to not run at the same time The examples below all focus on ensuring different instances (or threads) don't do the same work twice. E.g. don't try to create the same parent folder record twice. However, there are other times you may need to use a distributed lock with different behaviour. Specifically you may need to run two different methods that access the same record. Both methods need to run, but it is crucial that they do not run at the same time. In this case, you can use the AwaitLock method, as illustrated below. The crucial point with this example is that both pieces of code will run, but they will not run at the same time . // thread / microservice 1 var lockExpiry = TimeSpan.FromMinutes(5); var waitTimeout = TimeSpan.FromMinutes(2); using (var token = _distributedLock.AwaitLock(lockToken, lockExpiry, waitTimeout)) { // run piece of code 1 } // thread / microservice 2 using (var token = _distributedLock.AwaitLock(lockToken, lockExpiry, waitTimeout)) { // run piece of code 2 } Note the waitTimeout parameter. If it takes longer than this amount of time to acquire the lock, an exception will be thrown. The following screenshot with some debug logging active illustrates how this works.","title":"Distributed Locks"},{"location":"6.-Engineering/Structure-and-Patterns/Distributed-Locks/#using-the-distributed-lock-in-c-code","text":"You will need to dependency inject an IDistributedLock object into your service, as below: private readonly IDistributedLock _distributedLock; public MyService(IDistributedLock distributedLock) { _distributedLock = distributedLock; } There are three patterns for using the distributed lock. In each case you need to generate a predictable key, and you will also need to specify how long the lock is valid for (which guards against the lock never being released if the calling code dies). string lockKey = $\"asset.Create.{_client.ClientKey}.{asset.Id}\"; // only excecute the following code if you can acquire the lock // the lock is not explicitly released, it will expire after the specified time if (_distributedLock.AcquireLock(lockKey, TimeSpan.FromMinutes(5))) { // code to execute } // attempt to acquire the lock // you MUST explicitly check the HasLock property using (var token = _distributedLock.UsingToken(lockKey, TimeSpan.FromMinutes(5))) { if (token.HasLock) { // code to execute } else { // what to do when we do NOT have the lock? Probably nothing! } } // lock is explicitly released at the end of the using block // similar to the above, but using a lambda // If the lock cannot be acquired, it will simply not do anything await _distributedLock.RunAsync(lockKey, async () => { // code to execute }, 300);","title":"Using the Distributed Lock in C# code"},{"location":"6.-Engineering/Structure-and-Patterns/Distributed-Locks/#running-different-code-that-access-the-same-resource","text":"NOTE : use this approach to allow two separate processes to run, but guaranteed to not run at the same time The examples below all focus on ensuring different instances (or threads) don't do the same work twice. E.g. don't try to create the same parent folder record twice. However, there are other times you may need to use a distributed lock with different behaviour. Specifically you may need to run two different methods that access the same record. Both methods need to run, but it is crucial that they do not run at the same time. In this case, you can use the AwaitLock method, as illustrated below. The crucial point with this example is that both pieces of code will run, but they will not run at the same time . // thread / microservice 1 var lockExpiry = TimeSpan.FromMinutes(5); var waitTimeout = TimeSpan.FromMinutes(2); using (var token = _distributedLock.AwaitLock(lockToken, lockExpiry, waitTimeout)) { // run piece of code 1 } // thread / microservice 2 using (var token = _distributedLock.AwaitLock(lockToken, lockExpiry, waitTimeout)) { // run piece of code 2 } Note the waitTimeout parameter. If it takes longer than this amount of time to acquire the lock, an exception will be thrown. The following screenshot with some debug logging active illustrates how this works.","title":"Running different code that access the same resource"},{"location":"6.-Engineering/Structure-and-Patterns/Lean-Entrypoints/","text":"[[ TOC ]] Code Structure All the C# code we write for microservices runs in one of the following contexts, referred to here as different types of \"entry point\". They are all different ways of \"getting stuff done\" within the platform. HTTP Endpoint (or Microservice endpoint, or Microservice request) Background Task Message Queue handler Regular Task Each type of entry point serves a slightly different use-case, and should follow the appropriate conventions and patterns, which are described below. Ideally, all entry points should be very lean, and should call off to an ISomethingService (also called an IOC Service, after the Inversion of Control Pattern) to perform the bulk of the work. The advantage of this approach is that Unit Tests can focus on testing the IOC service, whilst the entry points stay very \"dumb\". HTTP Endpoints HTTP endpoints (aka. microservice endpoints) are the main way of starting an action in the platform, and most developers will be very familiar working with them. They are the only type of entry point that block untill they are finished, and the only type that provide a response. So if you need to retrieve a piece of information, an HTTP endpoint is the only option. Virtually all endpoints in Quartex are implemented as actions within MVC Controllers. Endpoint Best Practices Follow the endpoint naming best practices Controllers should be very lean: Collect the input from the request Perform basic validation (e.g. are all required parameters correct? More complex validation e.g. does an asset exist for a given ID should belong in the controller) Call an IOC Service to perform the meat of the request Minimal code to return the result from the IOC Service Use new ServiceResult() to wrap up the C# result object into an HTTP response The ideal endpoint code would look like this. In practice, it may be necessary to process some of the parameters somewhat before passing them to the IOC service. [HttpGet(\"endpoint\")] public Task<IActionResult> Endpoint(SomeRequest request) { var response = await _service.DoSomething(request); return new ServiceResult(response); } The service method should do all the error trapping and ensure the returned response object sets the Status property appropriately based on success or failure more. The advantage of ths approach is that the error trapping can be effectively Unit Tested. Background Tasks Background tasks in Quartex use the HangFire library. Background tasks will be kicked off by making a microservice request to the microservice that runs the task. The endpoint will then add the background task to the queue via Hangfire. Background Tasks are well suited to use-cases where there is either one single large job that needs to be done, or there are lots of items but they need to be done in a specific order, or can influence each other. Background Task Best Practices Background tasks have specific concerns around silent failures, retries and updating the user on the process of the task. Create an entry on the processes page to keep the user updated Avoid creating duplicate jobs Ensure the job is \"finished\" whether in success or failure Ensure that failures are detected Cater for long running tasks Following the pattern below should ensure all the concerns are dealt with: public async Task Run(TaskParameters parameters, IJobCancellationToken cancellationToken) { var context = parameters.GetContext(); try { await service.DoTheWork(parameters, context); } catch (Exception e) { if (!context.LastAttempt) { // If there are more attempts to come, log the error and re-throw the exception to trigger the next retry _logger.LogError(e, \"Error DOING SOMETHING for {ClientKey}: {Retries} retries\", parameters.ClientKey, context.Retry); throw; } else { // Otherwise ensure the UI is updated await _serviceClient.Request<UpdateJobRequest, UpdateJobResponse>(new UpdateJobRequest { ClientKey = parameters.ClientKey, JobId = parameters.JobId, Status = BatchJobStatus.CompleteWithErrors }); } } } Firstly, ensure that the endpoint that queues the task up via hangfire is what creates the Job on the processes page, and the JobId passed in via the job's parameters. This ensures that only one job is created. If the Job is created within the background task itself, there is the risk that should the task fail on the first attempt, subsequent attempts will create duplicate jobs. Next, the BackgroundTaskContext object (which can be retrieved from the task parameters) provides information about how many attempts have been made at the job so far. The pattern demonstrated above with the try/catch and a check against the LastAttempt property should ensure: The task is re-attempted where possible The Job on the processes page is updated should all attempts fail Long running background tasks: the re-entrant pattern Another specific concern with Background Tasks that take a long time is the possibility of the microservice restarting (or being scaled down) whilst the task is still running. A Background Task should process a single batch of input, meaning it will only run for a limited amount of time and has a good chance of processing the batch it is responsible for before potentially getting restarted. As such, the TaskParameters object will need a parameter such as BatchNumber or Skip for example, so it knows what work to do. When the background task finishes, it would then kick off the next iteration of itself (with an updated BatchNumber or Skip parameter as appropriate), which will process the next batch of items, and so on and so forth until the last item is processed. Each task would be responsible for updating the status/percentage of the job on the processes page, and the last job would be the one to set the status to Complete or Failed. This is an example of the ISomethingService method which does the real work of the background task: public async Task DoTheWork(TaskParameters parameters, BackgroundTaskContext context) { if (parameters.Skip == 0) { // this will only run on the first iteration } // var allItems = _otherService.GetAllItems(); var batch = allItems.Skip(parameters.Skip).Take(_batchSize); // Process items in the batch foreach (var item in batch) { await ProcessItem(item); } if (parameters.Skip + _batchSize >= allItems.Count()) { // finished case _serviceClient.Request<UpdateJobRequest, UpdateJobResponse>(new UpdateJobRequest { ClientKey = parameters.ClientKey, JobId = parameters.JobId, Status = BatchJobStatus.Complete }); } else { // kick off next batch parameters.Skip += _batchSize; _backgroundTasks.Start<ITask, ITaskParameters>(parameters); } } One thing to be careful of in this pattern is how we retrieve the items to be worked on. It may be a good idea to retrieve the list of items and store them in the cache for faster retrieval. Likewise, if the act of processing items would change the results of the query we make to retrieve the list of items in the first place, that can be problematic. Imagine a method called GetAllIncompleteAssets() in a task that needs to mark those assets as Complete . The very act of marking some of them complete will mean that future calls to GetAllIncompleteAssets() will return fewer items... this would be another good instance to store the list of items in the cache for all future iterations of the background task to process. Message Queue Handlers Message Queue handlers wait for messages to arrive on the queue and process each item independently. The MQ is well suited to use-cases where there are lots of very small items to be processed (like \"OCR this single image\") and when each item can be processed in any order without impacting any others, or for small discrete jobs that won't take very long. If an MQ handler might feasibly take more than a minute, it is probably better suited to being refactored as a Background Task. MQ Handler Best Practices MQ handlers should live in a controller class, and can be considered quite similar to HTTP endpoints, with the exception that they don't return a response as such. MQ Handlers should be very lean Collect the input from the message Call an IOC Service to perform the real work Minimal code to return success or failure Ensure that errors are caught and the appropriate status is returned MQ handlers, like background tasks work in the background (with less visibility to an end user), so as a result it is important that they are retried in the event of a failure or exception. It is however possible to return HandleMessageStatus.FailNoRetry to skip retries in the event that the piece of work being requested is invalid (e.g. validation error, missing parameters etc). The below is an example of how to implement an MQ handler that follows the above principles: public async Task<HandleMessageStatus> UpdateSingleItem(UpdateItemMessage message, MessageContext context) { try { await _updateSingleItem(message.ClientKey, message.ItemId); } catch (InvalidOperationException ioe) { _logger.LogError(ioe, \"Invalid message on the Queue for item with Id {ItemId} for {ClientKey}: {Error}\", message.ItemId, message.ClientKey, ioe.Message); return HandleMessageStatus.FailNoRetry; } catch (Exception e) { _logger.LogError(e, \"Could not update single item with Id {ItemId} for {ClientKey}\", message.ItemId, message.ClientKey); return HandleMessageStatus.Failed; } return HandleMessageStatus.Success; } Regular Tasks Background tasks in Quartex use the IHostedService feature of dotnet core . As with all other types of Entry Point, Regular Tasks should be very skinny and most of the work should be done in an ISomethingService .","title":"Lean Entrypoints"},{"location":"6.-Engineering/Structure-and-Patterns/Lean-Entrypoints/#code-structure","text":"All the C# code we write for microservices runs in one of the following contexts, referred to here as different types of \"entry point\". They are all different ways of \"getting stuff done\" within the platform. HTTP Endpoint (or Microservice endpoint, or Microservice request) Background Task Message Queue handler Regular Task Each type of entry point serves a slightly different use-case, and should follow the appropriate conventions and patterns, which are described below. Ideally, all entry points should be very lean, and should call off to an ISomethingService (also called an IOC Service, after the Inversion of Control Pattern) to perform the bulk of the work. The advantage of this approach is that Unit Tests can focus on testing the IOC service, whilst the entry points stay very \"dumb\".","title":"Code Structure"},{"location":"6.-Engineering/Structure-and-Patterns/Lean-Entrypoints/#http-endpoints","text":"HTTP endpoints (aka. microservice endpoints) are the main way of starting an action in the platform, and most developers will be very familiar working with them. They are the only type of entry point that block untill they are finished, and the only type that provide a response. So if you need to retrieve a piece of information, an HTTP endpoint is the only option. Virtually all endpoints in Quartex are implemented as actions within MVC Controllers.","title":"HTTP Endpoints"},{"location":"6.-Engineering/Structure-and-Patterns/Lean-Entrypoints/#endpoint-best-practices","text":"Follow the endpoint naming best practices Controllers should be very lean: Collect the input from the request Perform basic validation (e.g. are all required parameters correct? More complex validation e.g. does an asset exist for a given ID should belong in the controller) Call an IOC Service to perform the meat of the request Minimal code to return the result from the IOC Service Use new ServiceResult() to wrap up the C# result object into an HTTP response The ideal endpoint code would look like this. In practice, it may be necessary to process some of the parameters somewhat before passing them to the IOC service. [HttpGet(\"endpoint\")] public Task<IActionResult> Endpoint(SomeRequest request) { var response = await _service.DoSomething(request); return new ServiceResult(response); } The service method should do all the error trapping and ensure the returned response object sets the Status property appropriately based on success or failure more. The advantage of ths approach is that the error trapping can be effectively Unit Tested.","title":"Endpoint Best Practices"},{"location":"6.-Engineering/Structure-and-Patterns/Lean-Entrypoints/#background-tasks","text":"Background tasks in Quartex use the HangFire library. Background tasks will be kicked off by making a microservice request to the microservice that runs the task. The endpoint will then add the background task to the queue via Hangfire. Background Tasks are well suited to use-cases where there is either one single large job that needs to be done, or there are lots of items but they need to be done in a specific order, or can influence each other.","title":"Background Tasks"},{"location":"6.-Engineering/Structure-and-Patterns/Lean-Entrypoints/#background-task-best-practices","text":"Background tasks have specific concerns around silent failures, retries and updating the user on the process of the task. Create an entry on the processes page to keep the user updated Avoid creating duplicate jobs Ensure the job is \"finished\" whether in success or failure Ensure that failures are detected Cater for long running tasks Following the pattern below should ensure all the concerns are dealt with: public async Task Run(TaskParameters parameters, IJobCancellationToken cancellationToken) { var context = parameters.GetContext(); try { await service.DoTheWork(parameters, context); } catch (Exception e) { if (!context.LastAttempt) { // If there are more attempts to come, log the error and re-throw the exception to trigger the next retry _logger.LogError(e, \"Error DOING SOMETHING for {ClientKey}: {Retries} retries\", parameters.ClientKey, context.Retry); throw; } else { // Otherwise ensure the UI is updated await _serviceClient.Request<UpdateJobRequest, UpdateJobResponse>(new UpdateJobRequest { ClientKey = parameters.ClientKey, JobId = parameters.JobId, Status = BatchJobStatus.CompleteWithErrors }); } } } Firstly, ensure that the endpoint that queues the task up via hangfire is what creates the Job on the processes page, and the JobId passed in via the job's parameters. This ensures that only one job is created. If the Job is created within the background task itself, there is the risk that should the task fail on the first attempt, subsequent attempts will create duplicate jobs. Next, the BackgroundTaskContext object (which can be retrieved from the task parameters) provides information about how many attempts have been made at the job so far. The pattern demonstrated above with the try/catch and a check against the LastAttempt property should ensure: The task is re-attempted where possible The Job on the processes page is updated should all attempts fail","title":"Background Task Best Practices"},{"location":"6.-Engineering/Structure-and-Patterns/Lean-Entrypoints/#long-running-background-tasks-the-re-entrant-pattern","text":"Another specific concern with Background Tasks that take a long time is the possibility of the microservice restarting (or being scaled down) whilst the task is still running. A Background Task should process a single batch of input, meaning it will only run for a limited amount of time and has a good chance of processing the batch it is responsible for before potentially getting restarted. As such, the TaskParameters object will need a parameter such as BatchNumber or Skip for example, so it knows what work to do. When the background task finishes, it would then kick off the next iteration of itself (with an updated BatchNumber or Skip parameter as appropriate), which will process the next batch of items, and so on and so forth until the last item is processed. Each task would be responsible for updating the status/percentage of the job on the processes page, and the last job would be the one to set the status to Complete or Failed. This is an example of the ISomethingService method which does the real work of the background task: public async Task DoTheWork(TaskParameters parameters, BackgroundTaskContext context) { if (parameters.Skip == 0) { // this will only run on the first iteration } // var allItems = _otherService.GetAllItems(); var batch = allItems.Skip(parameters.Skip).Take(_batchSize); // Process items in the batch foreach (var item in batch) { await ProcessItem(item); } if (parameters.Skip + _batchSize >= allItems.Count()) { // finished case _serviceClient.Request<UpdateJobRequest, UpdateJobResponse>(new UpdateJobRequest { ClientKey = parameters.ClientKey, JobId = parameters.JobId, Status = BatchJobStatus.Complete }); } else { // kick off next batch parameters.Skip += _batchSize; _backgroundTasks.Start<ITask, ITaskParameters>(parameters); } } One thing to be careful of in this pattern is how we retrieve the items to be worked on. It may be a good idea to retrieve the list of items and store them in the cache for faster retrieval. Likewise, if the act of processing items would change the results of the query we make to retrieve the list of items in the first place, that can be problematic. Imagine a method called GetAllIncompleteAssets() in a task that needs to mark those assets as Complete . The very act of marking some of them complete will mean that future calls to GetAllIncompleteAssets() will return fewer items... this would be another good instance to store the list of items in the cache for all future iterations of the background task to process.","title":"Long running background tasks: the re-entrant pattern"},{"location":"6.-Engineering/Structure-and-Patterns/Lean-Entrypoints/#message-queue-handlers","text":"Message Queue handlers wait for messages to arrive on the queue and process each item independently. The MQ is well suited to use-cases where there are lots of very small items to be processed (like \"OCR this single image\") and when each item can be processed in any order without impacting any others, or for small discrete jobs that won't take very long. If an MQ handler might feasibly take more than a minute, it is probably better suited to being refactored as a Background Task.","title":"Message Queue Handlers"},{"location":"6.-Engineering/Structure-and-Patterns/Lean-Entrypoints/#mq-handler-best-practices","text":"MQ handlers should live in a controller class, and can be considered quite similar to HTTP endpoints, with the exception that they don't return a response as such. MQ Handlers should be very lean Collect the input from the message Call an IOC Service to perform the real work Minimal code to return success or failure Ensure that errors are caught and the appropriate status is returned MQ handlers, like background tasks work in the background (with less visibility to an end user), so as a result it is important that they are retried in the event of a failure or exception. It is however possible to return HandleMessageStatus.FailNoRetry to skip retries in the event that the piece of work being requested is invalid (e.g. validation error, missing parameters etc). The below is an example of how to implement an MQ handler that follows the above principles: public async Task<HandleMessageStatus> UpdateSingleItem(UpdateItemMessage message, MessageContext context) { try { await _updateSingleItem(message.ClientKey, message.ItemId); } catch (InvalidOperationException ioe) { _logger.LogError(ioe, \"Invalid message on the Queue for item with Id {ItemId} for {ClientKey}: {Error}\", message.ItemId, message.ClientKey, ioe.Message); return HandleMessageStatus.FailNoRetry; } catch (Exception e) { _logger.LogError(e, \"Could not update single item with Id {ItemId} for {ClientKey}\", message.ItemId, message.ClientKey); return HandleMessageStatus.Failed; } return HandleMessageStatus.Success; }","title":"MQ Handler Best Practices"},{"location":"6.-Engineering/Structure-and-Patterns/Lean-Entrypoints/#regular-tasks","text":"Background tasks in Quartex use the IHostedService feature of dotnet core . As with all other types of Entry Point, Regular Tasks should be very skinny and most of the work should be done in an ISomethingService .","title":"Regular Tasks"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/","text":"Overview Describe the general purpose of the Tech Roadmap Ideas are validated and added to roadmap -> Roadmap items are broken down into backlog items (describe item types, add examples) Describe who owns and contributes to it and who it is for Discovery Technology Vision and Strategy Describe the purpose of the Vision and strategy and link to it Capturing Ideas and Creating Roadmap Items At any point any member of the platform development team can (and should!) add new ideas to the technology ideation repository. Each of these ideas needs to go through some degree of validation This will depend upon the estimated complexity and scale of the idea Validation is performed using the \"Technology Canvas\" Each item will have a lead assigned Validation means ensuring it is valuable, feasible, and aligned with our vision and strategy Create an Idea item and complete as much of the canvas as possible Ideas are reviewed by the engineering team periodically (practice TBC), during which early prioritisation is performed and a lead is assigned The lead will then continue the validation process and complete the canvas If it is determined not be be valuable, viable, feasible, the reasons for this are captured, and no further work is done Otherwise it will be shared with the Technology Governance Group (TGG), who will evaluate and feedback, offering guidance for further validation. The TGG may also veto. Once the TGG are aligned with the canvas the Lead will estimate, rank, prioritise, and set a delivery target, utilising other members of the team to do this. Then add to the roadmap. (TLDR: any validated idea that has a delivery target, is essentially on the roadmap) The TGG will review the roadmap and feedback on, or adjust priority Guidelines for Technology Governance Group When evaluating canvases, feedback should help them further improve the validation they have performed, rather than directly offering a different opinion This will allow them to improve their ability to validate in the future None the less, the TGG may veto the idea regardless of the conclusions/suggestions made, if it is deemed tangential to the vision and mission The TGG need to ensure they set aside time to support the discovery process The TGG should organise regular sessions to review the roadmap priorities. Definition Similar to definition practice for Product BLIs The lead for the Technical Roadmap Item (TRI) will typically be the same person who was lead for the Idea, but this will not always be possible or appropriate The TRI lead should convene a \"3 Amigos\" group (or an \"N Amigos\" group) to provide input from Technical, Testing, Design, Security (etc) perspectives as appropriate to the TRI TRIs are broken down into one or more backlog items (BLIs), typically these will be \"Technical Improvements\" During the definition phase we explore and define overall solution we are aiming for, then turn this into a number of distinct deliverables At appropriate points within the definition process, the Software Architect should be consulted, who can provide guidance and feedback as to how the solution fits within the overall platform. As with product BLIs, the technical improvement BLIs should contain an overview of the desired technical solution and architectural design. If it is determined that we have insufficient knowledge to proceed, the N Amigos group can create an Investigation BLI to find any unknowns. The investigation should have a well defined outcome, to ensure the result means we can progress with definition thereafter. Each BLI should then be reviewed and estimated Each backlog item does not need to be delivered at the same time! We can add value gradually. This plan should be captured in the parent Roadmap Item. // TODO: Define actual mechanisms for assigning leads, reviewing backlog/ideas repo Define \"ready\" Summarise practice for getting work into sprints i.e. pre-planning Describe the different \"Technical Value Types\"","title":"Overview"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/#overview","text":"Describe the general purpose of the Tech Roadmap Ideas are validated and added to roadmap -> Roadmap items are broken down into backlog items (describe item types, add examples) Describe who owns and contributes to it and who it is for","title":"Overview"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/#discovery","text":"","title":"Discovery"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/#technology-vision-and-strategy","text":"Describe the purpose of the Vision and strategy and link to it","title":"Technology Vision and Strategy"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/#capturing-ideas-and-creating-roadmap-items","text":"At any point any member of the platform development team can (and should!) add new ideas to the technology ideation repository. Each of these ideas needs to go through some degree of validation This will depend upon the estimated complexity and scale of the idea Validation is performed using the \"Technology Canvas\" Each item will have a lead assigned Validation means ensuring it is valuable, feasible, and aligned with our vision and strategy Create an Idea item and complete as much of the canvas as possible Ideas are reviewed by the engineering team periodically (practice TBC), during which early prioritisation is performed and a lead is assigned The lead will then continue the validation process and complete the canvas If it is determined not be be valuable, viable, feasible, the reasons for this are captured, and no further work is done Otherwise it will be shared with the Technology Governance Group (TGG), who will evaluate and feedback, offering guidance for further validation. The TGG may also veto. Once the TGG are aligned with the canvas the Lead will estimate, rank, prioritise, and set a delivery target, utilising other members of the team to do this. Then add to the roadmap. (TLDR: any validated idea that has a delivery target, is essentially on the roadmap) The TGG will review the roadmap and feedback on, or adjust priority","title":"Capturing Ideas and Creating Roadmap Items"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/#guidelines-for-technology-governance-group","text":"When evaluating canvases, feedback should help them further improve the validation they have performed, rather than directly offering a different opinion This will allow them to improve their ability to validate in the future None the less, the TGG may veto the idea regardless of the conclusions/suggestions made, if it is deemed tangential to the vision and mission The TGG need to ensure they set aside time to support the discovery process The TGG should organise regular sessions to review the roadmap priorities.","title":"Guidelines for Technology Governance Group"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/#definition","text":"Similar to definition practice for Product BLIs The lead for the Technical Roadmap Item (TRI) will typically be the same person who was lead for the Idea, but this will not always be possible or appropriate The TRI lead should convene a \"3 Amigos\" group (or an \"N Amigos\" group) to provide input from Technical, Testing, Design, Security (etc) perspectives as appropriate to the TRI TRIs are broken down into one or more backlog items (BLIs), typically these will be \"Technical Improvements\" During the definition phase we explore and define overall solution we are aiming for, then turn this into a number of distinct deliverables At appropriate points within the definition process, the Software Architect should be consulted, who can provide guidance and feedback as to how the solution fits within the overall platform. As with product BLIs, the technical improvement BLIs should contain an overview of the desired technical solution and architectural design. If it is determined that we have insufficient knowledge to proceed, the N Amigos group can create an Investigation BLI to find any unknowns. The investigation should have a well defined outcome, to ensure the result means we can progress with definition thereafter. Each BLI should then be reviewed and estimated Each backlog item does not need to be delivered at the same time! We can add value gradually. This plan should be captured in the parent Roadmap Item. // TODO: Define actual mechanisms for assigning leads, reviewing backlog/ideas repo Define \"ready\" Summarise practice for getting work into sprints i.e. pre-planning Describe the different \"Technical Value Types\"","title":"Definition"},{"location":"7.-Technology-Vision-and-Strategy/Technology-Vision-and-Strategy/","text":"Who is this for? This section describes the way in which we plan ahead to ensure our technology estate will continue to serve the business and product needs in the future. All members of Platform Development will have an interest in ensuring that this approach is fit for purpose, and engineers need to engage with the vision and strategy by offering their expertise and insights to help us define and refine the path ahead Vision Continually improve our effectiveness, by improving our ability to deliver value fast and sustainably, whilst meeting quality, security, stability and performance needs to increase the longevity of our technology while increasing the rate at which we deliver value Guiding Principals All engineers must engage and contribute towards our technology strategy, roadmap and backlog The Tech Leadership team will are here to help drive the strategy forwards, not dictate it Ideas are validated rapidly before being included in the strategy or added to the roadmap Engineers must work collaboratively to ensure roadmap items are prepared for sprints in a timely manner Finding the best approach and solution for the problem is more important than delivering it quickly: a problem solved quickly is not the same as a problem solved well. Contextual Overview Our Technology Vision guides our Tech Strategy. Our Technology Strategy describes what we aim to achieve in mid-to-long term, in line with our guiding vision The Technology Strategy informs our Tech Roadmap , a set of high level outcomes, which are broken down into discrete deliverables to form our Backlog . This sits alongside our Product and Support visions, strategies, roadmaps and backlogs Our departmental mission links them together.","title":"Who is this for?"},{"location":"7.-Technology-Vision-and-Strategy/Technology-Vision-and-Strategy/#who-is-this-for","text":"This section describes the way in which we plan ahead to ensure our technology estate will continue to serve the business and product needs in the future. All members of Platform Development will have an interest in ensuring that this approach is fit for purpose, and engineers need to engage with the vision and strategy by offering their expertise and insights to help us define and refine the path ahead","title":"Who is this for?"},{"location":"7.-Technology-Vision-and-Strategy/Technology-Vision-and-Strategy/#vision","text":"Continually improve our effectiveness, by improving our ability to deliver value fast and sustainably, whilst meeting quality, security, stability and performance needs to increase the longevity of our technology while increasing the rate at which we deliver value","title":"Vision"},{"location":"7.-Technology-Vision-and-Strategy/Technology-Vision-and-Strategy/#guiding-principals","text":"All engineers must engage and contribute towards our technology strategy, roadmap and backlog The Tech Leadership team will are here to help drive the strategy forwards, not dictate it Ideas are validated rapidly before being included in the strategy or added to the roadmap Engineers must work collaboratively to ensure roadmap items are prepared for sprints in a timely manner Finding the best approach and solution for the problem is more important than delivering it quickly: a problem solved quickly is not the same as a problem solved well.","title":"Guiding Principals"},{"location":"7.-Technology-Vision-and-Strategy/Technology-Vision-and-Strategy/#contextual-overview","text":"Our Technology Vision guides our Tech Strategy. Our Technology Strategy describes what we aim to achieve in mid-to-long term, in line with our guiding vision The Technology Strategy informs our Tech Roadmap , a set of high level outcomes, which are broken down into discrete deliverables to form our Backlog . This sits alongside our Product and Support visions, strategies, roadmaps and backlogs Our departmental mission links them together.","title":"Contextual Overview"}]}